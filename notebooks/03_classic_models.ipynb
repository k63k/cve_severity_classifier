{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47eca92b",
   "metadata": {},
   "source": [
    "# Notebook 03: Klassische Modelle mit TF-IDF\n",
    "\n",
    "Vergleich zweier linearer Baselines (Logistic Regression, Multinomial Naive Bayes) auf den in Notebook 02 erzeugten Textvarianten. Fokus: schneller, reproduzierbarer Benchmark + Interpretierbarkeit (Koeffizienten / log Wahrscheinlichkeiten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "WORD_VOCAB_MAX = 50000\n",
    "CHAR_VOCAB_MAX = 60000\n",
    "DATA_DIR = Path('../data/processed/')\n",
    "# Mögliche Varianten: 'raw', 'clean', 'raw_lemma', 'clean_lemma'\n",
    "VARIANT = 'clean'  # anpassen für Einzel-Lauf\n",
    "\n",
    "FILE_MAP = {\n",
    "    'raw': 'cves_processed_text_raw.csv',\n",
    "    'clean': 'cves_processed_text_clean.csv',\n",
    "    'raw_lemma': 'cves_processed_text_raw_lemma.csv',\n",
    "    'clean_lemma': 'cves_processed_text_clean_lemma.csv'\n",
    "}\n",
    "# TEXT_COL_MAP kann None setzen -> automatische Erkennung.\n",
    "TEXT_COL_MAP = {\n",
    "    'raw': None,\n",
    "    'clean': None,\n",
    "    'raw_lemma': None,\n",
    "    'clean_lemma': None\n",
    "}\n",
    "\n",
    "def detect_text_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [c for c in df.columns if c.startswith('description_')]\n",
    "    if not candidates:\n",
    "        candidates = [c for c in df.columns if 'description' in c]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"Keine Beschreibungsspalte gefunden. Spalten: {df.columns.tolist()}\")\n",
    "    # Wähle Spalte mit größter mittlerer Textlänge\n",
    "    best = max(candidates, key=lambda c: df[c].astype(str).str.len().mean())\n",
    "    return best\n",
    "\n",
    "variant_file = DATA_DIR / FILE_MAP[VARIANT]\n",
    "assert variant_file.exists(), f\"Variant file missing: {variant_file}\"\n",
    "print({'variant': VARIANT, 'file': str(variant_file)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8766e",
   "metadata": {},
   "source": [
    "## 1. Konfiguration & Variantenauswahl\n",
    "\n",
    "Legt globale Parameter (Random State, Vokabulargrenzen) fest und wählt eine konkrete Textvariante (`VARIANT`). Unterstützt: raw, clean, raw_lemma, clean_lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c047105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(variant_file)\n",
    "required_base = {'cve_id','severity','severity_id'}\n",
    "missing_base = required_base - set(df.columns)\n",
    "assert not missing_base, f'Missing columns: {missing_base}'\n",
    "\n",
    "# Textspalte identifizieren\n",
    "mapped = TEXT_COL_MAP.get(VARIANT)\n",
    "if mapped is not None and mapped in df.columns:\n",
    "    text_col = mapped\n",
    "else:\n",
    "    text_col = detect_text_column(df)\n",
    "print('Verwendete Textspalte:', text_col)\n",
    "\n",
    "print('Shape (original):', df.shape)\n",
    "print('Klassenverteilung (original):', df['severity_id'].value_counts().to_dict())\n",
    "\n",
    "X = df[text_col].astype(str).values\n",
    "y = df['severity_id'].values\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print({'train': len(X_train_text), 'test': len(X_test_text), 'text_col': text_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684c0ca",
   "metadata": {},
   "source": [
    "## 2. Daten laden & Train/Test Split\n",
    "\n",
    "Liest die ausgewählte Variantendatei, erkennt dynamisch die Textspalte, filtert nichts weiter und erstellt einen stratifizierten Train/Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEBUG detect_text_column output check passed for:', text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_features=WORD_VOCAB_MAX,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32\n",
    ")\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(3,5),\n",
    "    min_df=2,\n",
    "    max_features=CHAR_VOCAB_MAX,\n",
    "    lowercase=False,\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32\n",
    ")\n",
    "X_train_word = word_vectorizer.fit_transform(X_train_text)\n",
    "X_test_word = word_vectorizer.transform(X_test_text)\n",
    "X_train_char = char_vectorizer.fit_transform(X_train_text)\n",
    "X_test_char = char_vectorizer.transform(X_test_text)\n",
    "X_train = hstack([X_train_word, X_train_char]).tocsr()\n",
    "X_test = hstack([X_test_word, X_test_char]).tocsr()\n",
    "print({'X_train': X_train.shape, 'X_test': X_test.shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40997899",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vektorisierung (Wort + Zeichen)\n",
    "\n",
    "Zwei getrennte TF-IDF Repräsentationen (Word 1–2grams, Char 3–5grams) werden gebildet und dann horizontal zusammengeführt. Char n-grams fangen Subwort-/Muster (z.B. API, Datei-Endungen, Exploit-Signaturen) ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=500, solver='lbfgs', random_state=RANDOM_STATE)\n",
    "logreg.fit(X_train, y_train)\n",
    "log_preds = logreg.predict(X_test)\n",
    "log_acc = accuracy_score(y_test, log_preds)\n",
    "log_f1 = f1_score(y_test, log_preds, average='macro')\n",
    "results.append({'model': 'LogisticRegression', 'accuracy': log_acc, 'macro_f1': log_f1})\n",
    "print('LogReg:', {'accuracy': round(log_acc,4), 'macro_f1': round(log_f1,4)})\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_preds = nb.predict(X_test)\n",
    "nb_acc = accuracy_score(y_test, nb_preds)\n",
    "nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
    "results.append({'model': 'MultinomialNB', 'accuracy': nb_acc, 'macro_f1': nb_f1})\n",
    "print('MultinomialNB:', {'accuracy': round(nb_acc,4), 'macro_f1': round(nb_f1,4)})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\nErgebnisübersicht:')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d4f71",
   "metadata": {},
   "source": [
    "## 4. Baseline-Training der Modelle\n",
    "\n",
    "Trainiert Logistic Regression (multinomial) und Multinomial Naive Bayes auf der kombinierten TF-IDF Matrix. Bewertet mit Accuracy & Macro-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = results_df.sort_values('macro_f1', ascending=False).iloc[0]\n",
    "print('\\nBestes Modell nach macro_f1:', best_row['model'])\n",
    "if best_row['model'] == 'LogisticRegression':\n",
    "    best_preds = log_preds\n",
    "else:\n",
    "    best_preds = nb_preds\n",
    "print(classification_report(y_test, best_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33efc8e",
   "metadata": {},
   "source": [
    "### 4.1 Klassifikationsbericht (Bestes Modell)\n",
    "\n",
    "Erzeugt einen detaillierten Report (Precision/Recall/F1) für das leistungsstärkste Baseline-Modell nach Macro-F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e8717",
   "metadata": {},
   "source": [
    "### 5.1 Support-Filter für seltene Klassen\n",
    "\n",
    "Optional: Entfernt Klassen mit sehr geringem Test-Support (< `MIN_TEST_SUPPORT`) aus dem Bericht, um verzerrte Metriken / Warnungen zu reduzieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f972d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "VARIANT_LIST = ['raw','clean','raw_lemma','clean_lemma']\n",
    "comp_rows = []\n",
    "results_path_variants = _Path('../results/classic_variant_comparison.csv')\n",
    "\n",
    "for v in VARIANT_LIST:\n",
    "    f = DATA_DIR / FILE_MAP[v]\n",
    "    if not f.exists():\n",
    "        print(f\"[SKIP] Datei fehlt: {f}\")\n",
    "        continue\n",
    "    d = pd.read_csv(f)\n",
    "    # Dynamische Spaltenerkennung\n",
    "    tcol_map_entry = TEXT_COL_MAP.get(v)\n",
    "    if tcol_map_entry and tcol_map_entry in d.columns:\n",
    "        tcol = tcol_map_entry\n",
    "    else:\n",
    "        try:\n",
    "            tcol = detect_text_column(d)\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] Keine geeignete Textspalte in {f.name}: {e}\")\n",
    "            continue\n",
    "    if tcol not in d.columns:\n",
    "        print(f\"[SKIP] Spalte {tcol} fehlt in {f.name}\")\n",
    "        continue\n",
    "    Xv = d[tcol].astype(str).values\n",
    "    yv = d['severity_id'].values\n",
    "    # Stratified split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(Xv, yv, test_size=TEST_SIZE, stratify=yv, random_state=RANDOM_STATE)\n",
    "    # Vectorizer (gleiche Einstellungen wie Basislauf)\n",
    "    w_vec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=WORD_VOCAB_MAX, strip_accents='unicode', lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "    c_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), min_df=2, max_features=CHAR_VOCAB_MAX, lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "    t0 = time()\n",
    "    X_tr_w = w_vec.fit_transform(X_tr)\n",
    "    X_te_w = w_vec.transform(X_te)\n",
    "    X_tr_c = c_vec.fit_transform(X_tr)\n",
    "    X_te_c = c_vec.transform(X_te)\n",
    "    X_tr_all = hstack([X_tr_w, X_tr_c]).tocsr()\n",
    "    X_te_all = hstack([X_te_w, X_te_c]).tocsr()\n",
    "    vec_time = time() - t0\n",
    "\n",
    "    # Modelle\n",
    "    # LogReg\n",
    "    lr_m = LogisticRegression(max_iter=500, solver='lbfgs', random_state=RANDOM_STATE)\n",
    "    lr_m.fit(X_tr_all, y_tr)\n",
    "    lr_preds = lr_m.predict(X_te_all)\n",
    "    lr_acc = accuracy_score(y_te, lr_preds)\n",
    "    lr_f1 = f1_score(y_te, lr_preds, average='macro')\n",
    "    comp_rows.append({'variant': v, 'model':'LogisticRegression', 'accuracy': lr_acc, 'macro_f1': lr_f1, 'vectorize_time_sec': vec_time})\n",
    "\n",
    "    # NB\n",
    "    nb_m = MultinomialNB()\n",
    "    nb_m.fit(X_tr_all, y_tr)\n",
    "    nb_preds = nb_m.predict(X_te_all)\n",
    "    nb_acc = accuracy_score(y_te, nb_preds)\n",
    "    nb_f1 = f1_score(y_te, nb_preds, average='macro')\n",
    "    comp_rows.append({'variant': v, 'model':'MultinomialNB', 'accuracy': nb_acc, 'macro_f1': nb_f1, 'vectorize_time_sec': vec_time})\n",
    "\n",
    "comp_df = pd.DataFrame(comp_rows)\n",
    "if not comp_df.empty:\n",
    "    display(comp_df.sort_values(['macro_f1','accuracy'], ascending=False))\n",
    "    try:\n",
    "        if results_path_variants.exists():\n",
    "            prev = pd.read_csv(results_path_variants)\n",
    "            comp_df = pd.concat([prev, comp_df], ignore_index=True)\n",
    "        comp_df.to_csv(results_path_variants, index=False)\n",
    "        print('Variant Vergleich gespeichert unter', results_path_variants)\n",
    "    except Exception as e:\n",
    "        print('Speichern fehlgeschlagen:', e)\n",
    "else:\n",
    "    print('Keine Varianten verarbeitet (Dateien/Spalten fehlen?).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9df8e",
   "metadata": {},
   "source": [
    "## 5. Variant-Vergleich (Optional)\n",
    "\n",
    "Benchmark über alle vorhandenen Textvarianten (`raw`, `clean`, `raw_lemma`, `clean_lemma`) mit identischer Vektorisierung. Liefert schnelle Indikation, welche Vorverarbeitung den größten Nutzen bietet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c109d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "MIN_TEST_SUPPORT = 5  # Schwellwert anpassen\n",
    "\n",
    "# Nutzt bereits berechnete best_preds & y_test\n",
    "test_support = Counter(y_test)\n",
    "# Mapping severity_id -> behalten?\n",
    "keep_labels = {lab for lab,count in test_support.items() if count >= MIN_TEST_SUPPORT}\n",
    "\n",
    "mask = [lab in keep_labels for lab in y_test]\n",
    "filtered_true = y_test[mask]\n",
    "if 'best_preds' in globals():\n",
    "    filtered_pred = np.array(best_preds)[mask]\n",
    "else:\n",
    "    filtered_pred = np.array([])\n",
    "\n",
    "print('Original Klassenanzahl:', len(test_support))\n",
    "print('Gefiltert (>= support):', len(keep_labels))\n",
    "print('Verworfene Klassen:', set(test_support.keys()) - keep_labels)\n",
    "\n",
    "if len(filtered_true) and len(np.unique(filtered_pred)):\n",
    "    print(classification_report(filtered_true, filtered_pred, zero_division=0, digits=4))\n",
    "else:\n",
    "    print('Zu wenige verbleibende Klassen oder keine Vorhersagen nach Filter.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ddcff6",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Grid & Logging\n",
    "\n",
    "Kleiner kombinierter Grid über TF-IDF (word/char) Parameter und Modell-Hyperparameter (LogReg: C, NB: alpha). Ergebnisse werden in `results/classic_models_baseline.csv` protokolliert (append)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, os\n",
    "from itertools import product\n",
    "from pathlib import Path as _Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "RESULTS_DIR = _Path('../results')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH = RESULTS_DIR / 'classic_models_baseline.csv'\n",
    "\n",
    "# Re-use Original Daten (X_train_text, X_test_text, y_train, y_test)\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "word_ngrams_choices = [(1,1),(1,2)]\n",
    "char_ngrams_choices = [(3,5)]\n",
    "min_df_choices = [1,2]\n",
    "word_max_feats_choices = [20000, 50000]\n",
    "char_max_feats_choices = [30000]\n",
    "logreg_C_choices = [0.5, 1.0, 2.0]\n",
    "nb_alpha_choices = [0.5, 1.0]\n",
    "\n",
    "rows = []\n",
    "start_global = time.time()\n",
    "run_id_base = int(start_global)\n",
    "\n",
    "for (w_ng, c_ng, min_df, w_max, c_max) in product(word_ngrams_choices, char_ngrams_choices, min_df_choices, word_max_feats_choices, char_max_feats_choices):\n",
    "    # Vectorizer fit\n",
    "    word_vec = TfidfVectorizer(ngram_range=w_ng, min_df=min_df, max_features=w_max, strip_accents='unicode', lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "    char_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=c_ng, min_df=min_df, max_features=c_max, lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "    t0 = time.time()\n",
    "    X_tr_w = word_vec.fit_transform(X_train_text)\n",
    "    X_te_w = word_vec.transform(X_test_text)\n",
    "    X_tr_c = char_vec.fit_transform(X_train_text)\n",
    "    X_te_c = char_vec.transform(X_test_text)\n",
    "    X_tr = hstack([X_tr_w, X_tr_c]).tocsr()\n",
    "    X_te = hstack([X_te_w, X_te_c]).tocsr()\n",
    "    vec_time = time.time() - t0\n",
    "\n",
    "    # Logistic Regression variations\n",
    "    for C in logreg_C_choices:\n",
    "        model_name = 'LogisticRegression'\n",
    "        logreg = LogisticRegression(max_iter=400, solver='lbfgs', C=C, n_jobs=None, random_state=RANDOM_STATE)\n",
    "        t1 = time.time()\n",
    "        logreg.fit(X_tr, y_train_enc)\n",
    "        train_time = time.time() - t1\n",
    "        preds = logreg.predict(X_te)\n",
    "        acc = accuracy_score(y_test_enc, preds)\n",
    "        f1 = f1_score(y_test_enc, preds, average='macro')\n",
    "        rows.append({\n",
    "            'run_id': run_id_base,\n",
    "            'timestamp': time.time(),\n",
    "            'model': model_name,\n",
    "            'variant': VARIANT,\n",
    "            'word_ngrams': str(w_ng),\n",
    "            'char_ngrams': str(c_ng),\n",
    "            'min_df': min_df,\n",
    "            'word_max_features': w_max,\n",
    "            'char_max_features': c_max,\n",
    "            'C': C,\n",
    "            'alpha': None,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1,\n",
    "            'vectorize_time_sec': vec_time,\n",
    "            'train_time_sec': train_time,\n",
    "            'n_train': X_tr.shape[0],\n",
    "            'n_test': X_te.shape[0],\n",
    "            'notes': ''\n",
    "        })\n",
    "\n",
    "    # Naive Bayes variations\n",
    "    for alpha in nb_alpha_choices:\n",
    "        model_name = 'MultinomialNB'\n",
    "        nb = MultinomialNB(alpha=alpha)\n",
    "        t1 = time.time()\n",
    "        nb.fit(X_tr, y_train_enc)\n",
    "        train_time = time.time() - t1\n",
    "        preds = nb.predict(X_te)\n",
    "        acc = accuracy_score(y_test_enc, preds)\n",
    "        f1 = f1_score(y_test_enc, preds, average='macro')\n",
    "        rows.append({\n",
    "            'run_id': run_id_base,\n",
    "            'timestamp': time.time(),\n",
    "            'model': model_name,\n",
    "            'variant': VARIANT,\n",
    "            'word_ngrams': str(w_ng),\n",
    "            'char_ngrams': str(c_ng),\n",
    "            'min_df': min_df,\n",
    "            'word_max_features': w_max,\n",
    "            'char_max_features': c_max,\n",
    "            'C': None,\n",
    "            'alpha': alpha,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1,\n",
    "            'vectorize_time_sec': vec_time,\n",
    "            'train_time_sec': train_time,\n",
    "            'n_train': X_tr.shape[0],\n",
    "            'n_test': X_te.shape[0],\n",
    "            'notes': ''\n",
    "        })\n",
    "\n",
    "results_grid_df = pd.DataFrame(rows)\n",
    "print('Grid Rows:', results_grid_df.shape)\n",
    "if RESULTS_PATH.exists():\n",
    "    prev = pd.read_csv(RESULTS_PATH)\n",
    "    results_grid_df = pd.concat([prev, results_grid_df], ignore_index=True)\n",
    "results_grid_df.to_csv(RESULTS_PATH, index=False)\n",
    "results_grid_df.sort_values('macro_f1', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9fe2b",
   "metadata": {},
   "source": [
    "### 6.1 Feature Importance (LogReg)\n",
    "\n",
    "Extrahiert für das beste Logistic-Regression-Modell die wichtigsten (positiven & negativen) Tokens je Klasse anhand der Koeffizienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rekonstruiere zuletzt im Grid verwendete word_vec / char_vec wenn LogReg vorhanden war\n",
    "# (Für vollständige Reproduzierbarkeit: separaten Fit mit besten Parametern durchführen.)\n",
    "from math import isfinite\n",
    "\n",
    "if 'results_grid_df' in globals():\n",
    "    # Bestes LogReg Modell bestimmen\n",
    "    best_lr_row = results_grid_df[results_grid_df['model']=='LogisticRegression'].sort_values('macro_f1', ascending=False).head(1)\n",
    "    if not best_lr_row.empty:\n",
    "        r = best_lr_row.iloc[0]\n",
    "        print('Best LogReg Params:', r.to_dict())\n",
    "        w_ng = eval(r['word_ngrams'])\n",
    "        c_ng = eval(r['char_ngrams'])\n",
    "        min_df = int(r['min_df'])\n",
    "        w_max = int(r['word_max_features']) if isfinite(r['word_max_features']) else None\n",
    "        c_max = int(r['char_max_features']) if isfinite(r['char_max_features']) else None\n",
    "        C = float(r['C']) if r['C'] else 1.0\n",
    "\n",
    "        # Refit für transparente Pipeline\n",
    "        word_vec_best = TfidfVectorizer(ngram_range=w_ng, min_df=min_df, max_features=w_max, strip_accents='unicode', lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "        char_vec_best = TfidfVectorizer(analyzer='char_wb', ngram_range=c_ng, min_df=min_df, max_features=c_max, lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "        X_tr_w_best = word_vec_best.fit_transform(X_train_text)\n",
    "        X_te_w_best = word_vec_best.transform(X_test_text)\n",
    "        X_tr_c_best = char_vec_best.fit_transform(X_train_text)\n",
    "        X_te_c_best = char_vec_best.transform(X_test_text)\n",
    "        X_tr_best = hstack([X_tr_w_best, X_tr_c_best]).tocsr()\n",
    "        X_te_best = hstack([X_te_w_best, X_te_c_best]).tocsr()\n",
    "\n",
    "        logreg_best = LogisticRegression(max_iter=400, solver='lbfgs', C=C, random_state=RANDOM_STATE)\n",
    "        logreg_best.fit(X_tr_best, y_train_enc)\n",
    "\n",
    "        feature_names = list(word_vec_best.get_feature_names_out()) + list(char_vec_best.get_feature_names_out())\n",
    "        coefs = logreg_best.coef_  # shape [n_classes, n_features]\n",
    "        top_k = 15\n",
    "        class_importance = {}\n",
    "        for class_index, class_label in enumerate(le.classes_):\n",
    "            weights = coefs[class_index]\n",
    "            top_pos_idx = np.argsort(weights)[-top_k:][::-1]\n",
    "            top_neg_idx = np.argsort(weights)[:top_k]\n",
    "            class_importance[int(class_label)] = {\n",
    "                'top_positive': [(feature_names[i], float(weights[i])) for i in top_pos_idx],\n",
    "                'top_negative': [(feature_names[i], float(weights[i])) for i in top_neg_idx]\n",
    "            }\n",
    "        import json\n",
    "        imp_path = _Path('../results/logreg_feature_importance.json')\n",
    "        with open(imp_path, 'w') as f:\n",
    "            json.dump(class_importance, f, indent=2)\n",
    "        print('Feature Importance gespeichert unter', imp_path)\n",
    "    else:\n",
    "        print('Kein LogReg Ergebnis im Grid gefunden.')\n",
    "else:\n",
    "    print('results_grid_df nicht definiert - Grid vorher ausführen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3592418",
   "metadata": {},
   "source": [
    "## 7. Visualisierungen & Analyse\n",
    "\n",
    "Grafische Auswertung der Grid-Ergebnisse und Modellinterpretationen:\n",
    "- 7.1 Modellvergleich\n",
    "- 7.2 Hyperparameter-Einflüsse\n",
    "- 7.3 LogReg Feature Importance (Balken)\n",
    "- 7.4 Naive Bayes Top Tokens (log P(token|class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c84bb",
   "metadata": {},
   "source": [
    "### 7.1 Modellvergleich (Best Scores)\n",
    "\n",
    "Zeigt für jedes Modell das jeweils beste Ergebnis (Accuracy & Macro-F1) aus dem Grid / Baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689faf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "if 'results_grid_df' not in globals():\n",
    "    path = _Path('../results/classic_models_baseline.csv')\n",
    "    if path.exists():\n",
    "        results_grid_df = pd.read_csv(path)\n",
    "    else:\n",
    "        print('Keine Grid-Ergebnisse gefunden. Abschnitt übersprungen.')\n",
    "\n",
    "if 'results_grid_df' in globals():\n",
    "    # Aggregiere bestes Ergebnis je Modell\n",
    "    best_per_model = results_grid_df.sort_values('macro_f1', ascending=False).groupby('model', as_index=False).first()\n",
    "    display(best_per_model[['model','accuracy','macro_f1']])\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "    # Verwende hue=model + legend=False, damit zukünftige seaborn Versionen kein Warning werfen\n",
    "    sns.barplot(data=best_per_model, x='model', y='accuracy', hue='model', ax=axes[0], palette='Blues_d', legend=False)\n",
    "    axes[0].set_title('Accuracy (best pro Modell)')\n",
    "    sns.barplot(data=best_per_model, x='model', y='macro_f1', hue='model', ax=axes[1], palette='Greens_d', legend=False)\n",
    "    axes[1].set_title('Macro-F1 (best pro Modell)')\n",
    "    for ax in axes:\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{p.get_height():.3f}\", (p.get_x()+p.get_width()/2, p.get_height()), ha='center', va='bottom', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('results_grid_df nicht verfügbar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbfdf2",
   "metadata": {},
   "source": [
    "### 7.2 Hyperparameter-Einflüsse\n",
    "\n",
    "Untersucht den Einfluss einzelner Einstellungen (C, alpha, min_df, word ngram_range) auf die Macro-F1 Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a38250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_grid_df' in globals():\n",
    "    df_hp = results_grid_df.copy()\n",
    "    # Wort-Ngram Range als String -> vereinfachen\n",
    "    df_hp['word_ngrams'] = df_hp['word_ngrams'].astype(str)\n",
    "    df_hp['char_ngrams'] = df_hp['char_ngrams'].astype(str)\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=(12,8))\n",
    "    # C vs Macro-F1 (LogReg)\n",
    "    lr_df = df_hp[df_hp['model']=='LogisticRegression'].dropna(subset=['C'])\n",
    "    if not lr_df.empty:\n",
    "        sns.lineplot(data=lr_df, x='C', y='macro_f1', marker='o', ax=axes[0,0])\n",
    "        axes[0,0].set_title('LogReg: C vs Macro-F1')\n",
    "    else:\n",
    "        axes[0,0].text(0.5,0.5,'Keine LogReg Daten', ha='center')\n",
    "\n",
    "    # alpha vs Macro-F1 (NB)\n",
    "    nb_df = df_hp[df_hp['model']=='MultinomialNB'].dropna(subset=['alpha'])\n",
    "    if not nb_df.empty:\n",
    "        sns.lineplot(data=nb_df, x='alpha', y='macro_f1', marker='o', color='orange', ax=axes[0,1])\n",
    "        axes[0,1].set_title('NaiveBayes: alpha vs Macro-F1')\n",
    "    else:\n",
    "        axes[0,1].text(0.5,0.5,'Keine NB Daten', ha='center')\n",
    "\n",
    "    # min_df Effekt (aggregiert best per setting)\n",
    "    agg_min_df = df_hp.sort_values('macro_f1', ascending=False).groupby(['model','min_df'], as_index=False).first()\n",
    "    sns.barplot(data=agg_min_df, x='min_df', y='macro_f1', hue='model', ax=axes[1,0])\n",
    "    axes[1,0].set_title('min_df Effekt (best per model & min_df)')\n",
    "\n",
    "    # ngram range Effekt (word)\n",
    "    agg_ng = df_hp.sort_values('macro_f1', ascending=False).groupby(['model','word_ngrams'], as_index=False).first()\n",
    "    sns.barplot(data=agg_ng, x='word_ngrams', y='macro_f1', hue='model', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Word ngram_range Effekt')\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('results_grid_df nicht verfügbar für Hyperparameter-Plots.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bcd41",
   "metadata": {},
   "source": [
    "### 7.3 LogReg Feature Importance Visualisierung\n",
    "\n",
    "Barplots der Top positiven und negativen Tokens pro Klasse basierend auf den Koeffizienten des besten Logistic-Regression Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdf07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import isfinite\n",
    "\n",
    "imp_json_path = _Path('../results/logreg_feature_importance.json')\n",
    "if imp_json_path.exists():\n",
    "    with open(imp_json_path) as f:\n",
    "        imp_data = json.load(f)\n",
    "    n_classes = len(imp_data)\n",
    "    fig, axes = plt.subplots(n_classes, 2, figsize=(10, 3*n_classes))\n",
    "    if n_classes == 1:\n",
    "        axes = np.array([axes])\n",
    "    for idx, (cls, val) in enumerate(sorted(imp_data.items(), key=lambda x: int(x[0]))):\n",
    "        pos = val['top_positive']\n",
    "        neg = val['top_negative']\n",
    "        pos_df = pd.DataFrame(pos, columns=['token','weight']).sort_values('weight')\n",
    "        neg_df = pd.DataFrame(neg, columns=['token','weight']).sort_values('weight')\n",
    "        sns.barplot(data=pos_df, x='weight', y='token', hue='token', dodge=False, legend=False, ax=axes[idx,0])\n",
    "        axes[idx,0].set_title(f'Class {cls} Top Positive')\n",
    "        sns.barplot(data=neg_df, x='weight', y='token', hue='token', dodge=False, legend=False, ax=axes[idx,1])\n",
    "        axes[idx,1].set_title(f'Class {cls} Top Negative')\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('Keine logreg_feature_importance.json gefunden. Vorher Section 5.1 ausführen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760781e",
   "metadata": {},
   "source": [
    "### 7.4 Naive Bayes Top Tokens\n",
    "\n",
    "Refit des besten NB Modells und Darstellung der höchsten log P(token|class) Werte je Klasse – liefert probabilistische Sicht auf diskriminative Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit des besten NB Modells anhand Grid (analog zu LogReg Importance)\n",
    "if 'results_grid_df' in globals():\n",
    "    best_nb_row = results_grid_df[results_grid_df['model']=='MultinomialNB'].sort_values('macro_f1', ascending=False).head(1)\n",
    "    if not best_nb_row.empty:\n",
    "        r = best_nb_row.iloc[0]\n",
    "        print('Best NB Params:', r.to_dict())\n",
    "        from math import isfinite\n",
    "        w_ng = eval(r['word_ngrams'])\n",
    "        c_ng = eval(r['char_ngrams'])\n",
    "        min_df = int(r['min_df'])\n",
    "        w_max = int(r['word_max_features']) if isfinite(r['word_max_features']) else None\n",
    "        c_max = int(r['char_max_features']) if isfinite(r['char_max_features']) else None\n",
    "        alpha = float(r['alpha']) if r['alpha'] else 1.0\n",
    "\n",
    "        word_vec_nb = TfidfVectorizer(ngram_range=w_ng, min_df=min_df, max_features=w_max, strip_accents='unicode', lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "        char_vec_nb = TfidfVectorizer(analyzer='char_wb', ngram_range=c_ng, min_df=min_df, max_features=c_max, lowercase=False, sublinear_tf=True, dtype=np.float32)\n",
    "        X_tr_w_nb = word_vec_nb.fit_transform(X_train_text)\n",
    "        X_te_w_nb = word_vec_nb.transform(X_test_text)\n",
    "        X_tr_c_nb = char_vec_nb.fit_transform(X_train_text)\n",
    "        X_te_c_nb = char_vec_nb.transform(X_test_text)\n",
    "        X_tr_nb = hstack([X_tr_w_nb, X_tr_c_nb]).tocsr()\n",
    "        X_te_nb = hstack([X_te_w_nb, X_te_c_nb]).tocsr()\n",
    "\n",
    "        nb_best = MultinomialNB(alpha=alpha)\n",
    "        nb_best.fit(X_tr_nb, y_train_enc)\n",
    "\n",
    "        feature_names_nb = list(word_vec_nb.get_feature_names_out()) + list(char_vec_nb.get_feature_names_out())\n",
    "        log_prob = nb_best.feature_log_prob_  # shape [n_classes, n_features]\n",
    "        top_k = 15\n",
    "\n",
    "        fig, axes = plt.subplots(len(le.classes_), 1, figsize=(10, 3*len(le.classes_)))\n",
    "        if len(le.classes_) == 1:\n",
    "            axes = [axes]\n",
    "        nb_tokens_export = {}\n",
    "        for idx, class_label in enumerate(le.classes_):\n",
    "            weights = log_prob[idx]\n",
    "            top_idx = np.argsort(weights)[-top_k:][::-1]\n",
    "            top_tokens = [(feature_names_nb[i], float(weights[i])) for i in top_idx]\n",
    "            nb_tokens_export[int(class_label)] = top_tokens\n",
    "            plot_df = pd.DataFrame(top_tokens, columns=['token','log_prob']).sort_values('log_prob')\n",
    "            sns.barplot(data=plot_df, x='log_prob', y='token', hue='token', dodge=False, legend=False, ax=axes[idx])\n",
    "            axes[idx].set_title(f'NB log P(token|class={class_label}) Top {top_k}')\n",
    "        plt.tight_layout()\n",
    "        out_path = _Path('../results/nb_feature_logprob.json')\n",
    "        with open(out_path, 'w') as f:\n",
    "            json.dump(nb_tokens_export, f, indent=2)\n",
    "        print('NB Token Log-Prob exportiert nach', out_path)\n",
    "    else:\n",
    "        print('Kein NB Modell im Grid gefunden.')\n",
    "else:\n",
    "    print('results_grid_df nicht verfügbar – Grid zuerst ausführen.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
