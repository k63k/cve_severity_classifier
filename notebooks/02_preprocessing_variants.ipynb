{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54ba17d",
   "metadata": {},
   "source": [
    "# 02_preprocessing_variants_complete\n",
    "\n",
    "Vollständige konsolidierte Erstellung zweier Preprocessing-Varianten:\n",
    "\n",
    "1. **raw_minimal** – minimale Normalisierung (nur lowercase + Whitespace).\n",
    "2. **clean_lemma_stop** – vollständige Pipeline (Normalisierung, Acronym-Expansion, Stopwords, optional Lemmatization).\n",
    "\n",
    "Ausgabe CSV-Dateien (Ordner `data/processed/`):\n",
    "- `cves_processed_text_raw.csv`\n",
    "- `cves_processed_text_clean.csv`\n",
    "\n",
    "Inhalte direkt aus legacy Notebook übernommen – keine neuen Features hinzugefügt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8d567",
   "metadata": {},
   "source": [
    "## 1. Imports & Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccda3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidates': ['cves_v30.csv'], 'CSV_PATH': '../data/raw/cves_v30.csv'}\n"
     ]
    }
   ],
   "source": [
    "import re, html\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "USE_LEMMATIZATION = True  # optional (clean Variante)\n",
    "\n",
    "# Stopwords sicherstellen\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# spaCy optional\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser','ner']) if USE_LEMMATIZATION else None\n",
    "    except OSError:\n",
    "        print('Hinweis: en_core_web_sm nicht installiert')\n",
    "        nlp = None\n",
    "except ImportError:\n",
    "    if USE_LEMMATIZATION:\n",
    "        print('spaCy nicht installiert – Lemmatisierung deaktiviert')\n",
    "    nlp = None\n",
    "\n",
    "DATA_DIR = Path('..') / 'data' / 'raw'\n",
    "DEFAULT_CSV_LIST = ['cves_v40.csv','cves_v31.csv','cves_v30.csv','cves_v2.csv']\n",
    "CSV_SOURCE = 'cves_v30.csv'\n",
    "\n",
    "if CSV_SOURCE is None:\n",
    "    candidates = DEFAULT_CSV_LIST\n",
    "elif isinstance(CSV_SOURCE, str):\n",
    "    candidates = [CSV_SOURCE]\n",
    "elif isinstance(CSV_SOURCE, (list, tuple)):\n",
    "    candidates = list(CSV_SOURCE)\n",
    "else:\n",
    "    raise ValueError('CSV_SOURCE muss None, str oder Liste sein')\n",
    "CSV_PATH = next((DATA_DIR / c for c in candidates if (DATA_DIR / c).exists()), None)\n",
    "print({'candidates': candidates, 'CSV_PATH': str(CSV_PATH)})\n",
    "assert CSV_PATH is not None, 'Keine CVE CSV gefunden.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371eb268",
   "metadata": {},
   "source": [
    "## 2. Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f92253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen eingelesen: 53856\n",
      "          cve_id  severity                                        description\n",
      "0  CVE-2000-0258      HIGH  IIS 4.0 and 5.0 allows remote attackers to cau...\n",
      "1  CVE-2004-0847  CRITICAL  The Microsoft .NET forms authentication capabi...\n",
      "2  CVE-2005-0109    MEDIUM  Hyper-Threading technology, as used in FreeBSD...\n",
      "Nach Drop NA: 53856\n"
     ]
    }
   ],
   "source": [
    "USE_COLS = ['cve_id','severity','description']\n",
    "df = pd.read_csv(CSV_PATH, usecols=USE_COLS)\n",
    "print('Zeilen eingelesen:', len(df))\n",
    "print(df.head(3)[USE_COLS])\n",
    "df = df.dropna(subset=['description'])\n",
    "print('Nach Drop NA:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aed577",
   "metadata": {},
   "source": [
    "## 3. Acronym Expansion Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c437a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote code execution via server side request forgery causes local privilege escalation + cross site scripting & denial of service\n"
     ]
    }
   ],
   "source": [
    "ACRONYM_MAP = {\n",
    "    # Impact-/Exploit-Typen\n",
    "    'rce': 'remote code execution',\n",
    "    'codeexec': 'remote code execution',\n",
    "    'eop': 'elevation of privilege',\n",
    "    'privesc': 'privilege escalation',\n",
    "    'pe': 'privilege escalation',\n",
    "    'lpe': 'local privilege escalation',\n",
    "    'uep': 'unauthorized privilege escalation',\n",
    "    'dos': 'denial of service',\n",
    "    'crash': 'denial of service',\n",
    "    'ddos': 'distributed denial of service',\n",
    "    'infoleak': 'information disclosure',\n",
    "    'id': 'information disclosure',\n",
    "    'leak': 'information disclosure',\n",
    "\n",
    "    # Speicher-/Memory-Corruption\n",
    "    'bof': 'buffer overflow',\n",
    "    'bo': 'buffer overflow',\n",
    "    'heapof': 'heap overflow',\n",
    "    'heapovf': 'heap overflow',\n",
    "    'heapoverflow': 'heap overflow',\n",
    "    'stackof': 'stack overflow',\n",
    "    'stackoverflow': 'stack overflow',\n",
    "    'oob': 'out of bounds access',\n",
    "    'oobr': 'out of bounds read',\n",
    "    'oobw': 'out of bounds write',\n",
    "    'iob': 'index out of bounds',\n",
    "    'uaf': 'use after free',\n",
    "    'df': 'double free',\n",
    "    'npd': 'null pointer dereference',\n",
    "    'tc': 'type confusion',\n",
    "    'typeconf': 'type confusion',\n",
    "    'fmtstr': 'format string vulnerability',\n",
    "    'race': 'race condition',\n",
    "    'toctou': 'time of check to time of use',\n",
    "    'intof': 'integer overflow',\n",
    "    'intuf': 'integer underflow',\n",
    "    'sigrop': 'signal oriented programming',\n",
    "\n",
    "    # Web-/AppSec-Klassen\n",
    "    'xss': 'cross site scripting',\n",
    "    'xxs': 'cross site scripting',\n",
    "    'uxss': 'universal cross site scripting',\n",
    "    'csrf': 'cross site request forgery',\n",
    "    'xsrf': 'cross site request forgery',\n",
    "    'ssrf': 'server side request forgery',\n",
    "    'sqli': 'sql injection',\n",
    "    'nosqli': 'nosql injection',\n",
    "    'ldapi': 'ldap injection',\n",
    "    'ognl': 'ognl expression injection',\n",
    "    'cmdi': 'command injection',\n",
    "    'rce-inj': 'command injection',\n",
    "    'ssti': 'server side template injection',\n",
    "    'xssi': 'cross site script inclusion',\n",
    "    'xxe': 'xml external entity',\n",
    "    'xee': 'xml external entity',\n",
    "    'lfi': 'local file inclusion',\n",
    "    'rfi': 'remote file inclusion',\n",
    "    'idor': 'insecure direct object reference',\n",
    "    'bidor': 'blind insecure direct object reference',\n",
    "    'dirtrav': 'directory traversal',\n",
    "    'pathtrav': 'directory traversal',\n",
    "    'openredirect': 'open redirect',\n",
    "    'or': 'open redirect',\n",
    "    'crlfi': 'crlf injection',\n",
    "    'hpp': 'http parameter pollution',\n",
    "    'hhi': 'host header injection',\n",
    "    'reqsmuggle': 'http request smuggling',\n",
    "    'hrs': 'http request smuggling',\n",
    "    'reqsplit': 'http response splitting',\n",
    "    'cachepoison': 'web cache poisoning',\n",
    "    'deser': 'insecure deserialization',\n",
    "    'insecdeser': 'insecure deserialization',\n",
    "    'proto_pollution': 'prototype pollution',\n",
    "    'fileupload': 'insecure file upload',\n",
    "    'zipSlip': 'zip slip path traversal',\n",
    "\n",
    "    # AuthN/AuthZ/Session\n",
    "    'ato': 'account takeover',\n",
    "    '2fa': 'two factor authentication',\n",
    "    'mfa': 'multi factor authentication',\n",
    "    'sso': 'single sign on',\n",
    "    'saml': 'security assertion markup language',\n",
    "    'oauth': 'oauth',\n",
    "    'oidc': 'openid connect',\n",
    "    'jwt': 'json web token',\n",
    "    'jwts': 'json web token',\n",
    "    'jwk': 'json web key',\n",
    "    'jwks': 'json web key set',\n",
    "    'pkce': 'proof key for code exchange',\n",
    "    'bruteforce': 'credential brute force',\n",
    "    'credstuff': 'credential stuffing',\n",
    "\n",
    "    # Protokolle/HTTP-Sonderfälle\n",
    "    'wsx': 'websocket hijacking',\n",
    "    'dnsrb': 'dns rebinding',\n",
    "    'ssrf-blind': 'blind server side request forgery',\n",
    "\n",
    "    # Plattform-/Mitigations-/Exploitation-Techniques\n",
    "    'aslr': 'address space layout randomization',\n",
    "    'kaslr': 'kernel address space layout randomization',\n",
    "    'dep': 'data execution prevention',\n",
    "    'nx': 'no execute',\n",
    "    'rop': 'return oriented programming',\n",
    "    'jop': 'jump oriented programming',\n",
    "    'cfi': 'control flow integrity',\n",
    "    'cet': 'control-flow enforcement technology',\n",
    "    'pie': 'position independent executable',\n",
    "    'relro': 'relocation read-only',\n",
    "    'pac': 'pointer authentication',\n",
    "    'sandbox': 'process sandboxing',\n",
    "    'seccomp': 'secure computing mode',\n",
    "    'fortify': 'fortify source hardening',\n",
    "\n",
    "    # Cloud/Config/Secrets\n",
    "    'misconfig': 'security misconfiguration',\n",
    "    'openbucket': 'public cloud storage misconfiguration',\n",
    "    's3public': 'public cloud storage misconfiguration',\n",
    "    'imds': 'cloud instance metadata service exposure',\n",
    "    'k8srbac': 'kubernetes rbac misconfiguration',\n",
    "    'secretleak': 'secrets exposure',\n",
    "    'hardcodedcred': 'hardcoded credentials',\n",
    "\n",
    "    # Kataloge/Scoring/Taxonomien\n",
    "    'cve': 'common vulnerabilities and exposures',\n",
    "    'cwe': 'common weakness enumeration',\n",
    "    'cvss': 'common vulnerability scoring system',\n",
    "    'epss': 'exploit prediction scoring system',\n",
    "    'cpe': 'common platform enumeration',\n",
    "\n",
    "    # Sonstiges/Meta\n",
    "    'poc': 'proof of concept',\n",
    "    'exploit': 'exploit',\n",
    "    '0day': 'zero day',\n",
    "    'zeroday': 'zero day',\n",
    "    'nday': 'n day',\n",
    "}\n",
    "ACRONYM_REGEX = re.compile(r'\\b(' + '|'.join(map(re.escape, ACRONYM_MAP.keys())) + r')\\b', re.IGNORECASE)\n",
    "def expand_acronyms(text: str) -> str:\n",
    "    def repl(m):\n",
    "        return ACRONYM_MAP.get(m.group(1).lower(), m.group(1))\n",
    "    return ACRONYM_REGEX.sub(repl, text)\n",
    "print(expand_acronyms('RCE via SSRF causes LPE + XSS & DoS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd9309",
   "metadata": {},
   "source": [
    "## 4. Preprocessing-Funktionen (clean Variante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdf4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote code execution issue allow remote code execution cross site scripting via sql injection\n"
     ]
    }
   ],
   "source": [
    "PUNCT_REGEX = re.compile(r\"[\\\"'`´’“”‘()\\[\\]{}<>=:;,+*/\\\\|~^]\")\n",
    "MULTI_WS = re.compile(r'\\s{2,}')\n",
    "HTML_TAG = re.compile(r'<[^>]+>')\n",
    "DIGITS = re.compile(r'\\b\\d+\\b')\n",
    "NON_ALPHANUM = re.compile(r'[^a-z0-9 ]')\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = html.unescape(text)\n",
    "    text = HTML_TAG.sub(' ', text)\n",
    "    text = text.replace('\\t',' ').replace('\\n',' ')\n",
    "    text = PUNCT_REGEX.sub(' ', text)\n",
    "    text = DIGITS.sub(' ', text)\n",
    "    text = NON_ALPHANUM.sub(' ', text)\n",
    "    text = MULTI_WS.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    if not USE_LEMMATIZATION or nlp is None:\n",
    "        return tokens\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [tok.lemma_ for tok in doc]\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = normalize_basic(text)\n",
    "    text = expand_acronyms(text)\n",
    "    tokens = text.split(' ')\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(preprocess_text('This RCE issue allows Remote Code Execution and XSS via SQLi.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc7ccb",
   "metadata": {},
   "source": [
    "## 5. Anwenden Pipeline (clean Variante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abaa9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig: 53856 Zeilen preprocesset in 160.64s (Lemmatization=True)\n",
      "                                         description  \\\n",
      "0  IIS 4.0 and 5.0 allows remote attackers to cau...   \n",
      "1  The Microsoft .NET forms authentication capabi...   \n",
      "2  Hyper-Threading technology, as used in FreeBSD...   \n",
      "3  Microsoft w3wp (aka w3wp.exe) does not properl...   \n",
      "4  Cross-site scripting (XSS) vulnerability in in...   \n",
      "\n",
      "                                   description_clean  \n",
      "0  iis allow remote attacker cause denial service...  \n",
      "1  microsoft net form authentication capability a...  \n",
      "2  hyper threading technology use freebsd operati...  \n",
      "3  microsoft w3wp aka w3wp exe properly handle as...  \n",
      "4  cross site script cross site script vulnerabil...  \n",
      "Ø Tokens nach Preprocessing: 33.34\n",
      "Ø Tokens nach Preprocessing: 33.34\n"
     ]
    }
   ],
   "source": [
    "# Beschleunigte Anwendung der Pipeline (clean Variante) mit Batch-Verarbeitung\n",
    "import time\n",
    "START_TIME = time.time()\n",
    "\n",
    "# Optionale Begrenzung für schnellere Iterationen\n",
    "LIMIT_ROWS = None  # z.B. 5000 für schnellen Test\n",
    "BATCH_SIZE_SPACY = 400\n",
    "\n",
    "work_df = df if LIMIT_ROWS is None else df.head(LIMIT_ROWS).copy()\n",
    "texts = work_df['description'].astype(str).tolist()\n",
    "\n",
    "normalized = [normalize_basic(t) for t in texts]\n",
    "expanded = [expand_acronyms(t) for t in normalized]\n",
    "# Tokenisierung simpel per split (später für spaCy Lemmas ersetzt)\n",
    "token_lists = [t.split(' ') for t in expanded]\n",
    "\n",
    "# Stopwords entfernen\n",
    "filtered_tokens = [[tok for tok in toks if tok and tok not in STOPWORDS and len(tok) > 1] for toks in token_lists]\n",
    "\n",
    "if USE_LEMMATIZATION and nlp is not None:\n",
    "    # Batch Lemmatization nur auf gefilterten Tokens\n",
    "    joined = [' '.join(toks) for toks in filtered_tokens]\n",
    "    lemmas = []\n",
    "    for doc in nlp.pipe(joined, batch_size=BATCH_SIZE_SPACY, disable=['parser','ner']):\n",
    "        lemmas.append(' '.join(tok.lemma_ for tok in doc if tok.lemma_))\n",
    "    final_texts = lemmas\n",
    "else:\n",
    "    final_texts = [' '.join(toks) for toks in filtered_tokens]\n",
    "\n",
    "work_df['description_clean'] = final_texts\n",
    "\n",
    "elapsed = time.time() - START_TIME\n",
    "print(f'Fertig: {len(work_df)} Zeilen preprocesset in {elapsed:.2f}s (Lemmatization={USE_LEMMATIZATION and nlp is not None})')\n",
    "print(work_df[['description','description_clean']].head(5))\n",
    "print('Ø Tokens nach Preprocessing:', round(work_df['description_clean'].str.split().apply(len).mean(),2))\n",
    "\n",
    "# In Haupt-df zurückschreiben\n",
    "if LIMIT_ROWS is None:\n",
    "    df = work_df\n",
    "else:\n",
    "    # Nur subset ersetzt; Hinweis ausgeben\n",
    "    df.loc[work_df.index, 'description_clean'] = work_df['description_clean']\n",
    "    print('WARN: LIMIT_ROWS aktiv, nur Teilmenge verarbeitet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57bdb42",
   "metadata": {},
   "source": [
    "## 6. Severity Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb0704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwerfe unbekannte Severity Labels: ['none']\n",
      "Entfernt 1 Zeilen aufgrund unbekannter Labels.\n",
      "Severity Mapping: {'low': 0, 'medium': 1, 'high': 2, 'critical': 3}\n",
      "Verteilung: {2: 24128, 1: 20794, 3: 7497, 0: 1436}\n"
     ]
    }
   ],
   "source": [
    "ORDERED_SEVERITIES = ['low','medium','high','critical']\n",
    "sev_norm = df['severity'].astype(str).str.lower()\n",
    "unknown = sorted(set(sev_norm) - set(ORDERED_SEVERITIES))\n",
    "if unknown:\n",
    "    print('Verwerfe unbekannte Severity Labels:', unknown)\n",
    "    before = len(df)\n",
    "    df = df[sev_norm.isin(ORDERED_SEVERITIES)].copy()\n",
    "    sev_norm = df['severity'].astype(str).str.lower()\n",
    "    print(f'Entfernt {before - len(df)} Zeilen aufgrund unbekannter Labels.')\n",
    "severity_to_id = {s:i for i,s in enumerate(ORDERED_SEVERITIES)}\n",
    "severity_ids = sev_norm.map(severity_to_id)\n",
    "assert not severity_ids.isna().any(), 'NaN im Severity Mapping nach Filter'\n",
    "df['severity_id'] = severity_ids.astype(int)\n",
    "print('Severity Mapping:', severity_to_id)\n",
    "print('Verteilung:', df['severity_id'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3ce63",
   "metadata": {},
   "source": [
    "## 7. Basisstatistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739678af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       description_clean  severity\n",
      "21066  issue discover adobe acrobat reader early vers...    MEDIUM\n",
      "28248  argument injection vulnerability sourcetree ma...  CRITICAL\n",
      "50895  sensitive information disclosure due miss auth...    MEDIUM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_rows': 53855, 'n_unique_cve': 53855, 'avg_tokens_post': 33.338130164330146}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = {\n",
    "    'n_rows': int(len(df)),\n",
    "    'n_unique_cve': int(df['cve_id'].nunique()) if 'cve_id' in df.columns else None,\n",
    "    'avg_tokens_post': float(df['description_clean'].str.split().apply(len).mean()),\n",
    "}\n",
    "print(df[['description_clean','severity']].sample(min(3, len(df))))\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483909a",
   "metadata": {},
   "source": [
    "## 8. Varianten Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db72c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: entferne 1 Zeilen wegen unbekannter Severity Labels: ['none']\n",
      "Gespeichert text_raw -> cves_processed_text_raw.csv (53855, 4)\n",
      "Gespeichert text_clean -> cves_processed_text_clean.csv (53855, 4)\n",
      "Severity Mapping Export: {'low': 0, 'medium': 1, 'high': 2, 'critical': 3}\n",
      "Hinweis: Für zusätzliche Lemma-Varianten folgt nächste Zelle.\n",
      "Gespeichert text_raw -> cves_processed_text_raw.csv (53855, 4)\n",
      "Gespeichert text_clean -> cves_processed_text_clean.csv (53855, 4)\n",
      "Severity Mapping Export: {'low': 0, 'medium': 1, 'high': 2, 'critical': 3}\n",
      "Hinweis: Für zusätzliche Lemma-Varianten folgt nächste Zelle.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_OUT = OUTPUT_DIR / 'cves_processed_text_raw.csv'\n",
    "CLEAN_OUT = OUTPUT_DIR / 'cves_processed_text_clean.csv'\n",
    "base_df = df[['cve_id','severity','severity_id','description_clean']].copy()\n",
    "\n",
    "# Raw Variante erneut einlesen (minimales Cleaning + gleiche Severity-Filterlogik)\n",
    "raw_src = pd.read_csv(CSV_PATH, usecols=['cve_id','severity','description']).dropna()\n",
    "raw_src['severity'] = raw_src['severity'].astype(str)\n",
    "raw_src_norm = raw_src['severity'].str.lower()\n",
    "unknown_raw = sorted(set(raw_src_norm) - set(ORDERED_SEVERITIES))\n",
    "if unknown_raw:\n",
    "    before = len(raw_src)\n",
    "    raw_src = raw_src[raw_src_norm.isin(ORDERED_SEVERITIES)].copy()\n",
    "    raw_src_norm = raw_src['severity'].str.lower()\n",
    "    print(f'Raw: entferne {before - len(raw_src)} Zeilen wegen unbekannter Severity Labels: {unknown_raw}')\n",
    "severity_to_id_export = {s:i for i,s in enumerate(ORDERED_SEVERITIES)}\n",
    "raw_src['severity_id'] = raw_src_norm.map(severity_to_id_export)\n",
    "\n",
    "def minimal_clean(t: str) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip()\n",
    "raw_src['description_clean'] = raw_src['description'].apply(minimal_clean)\n",
    "raw_variant = raw_src[['cve_id','severity','severity_id','description_clean']].copy()\n",
    "raw_variant.to_csv(RAW_OUT, index=False)\n",
    "clean_variant = base_df.copy()\n",
    "clean_variant.to_csv(CLEAN_OUT, index=False)\n",
    "print('Gespeichert text_raw ->', RAW_OUT.name, raw_variant.shape)\n",
    "print('Gespeichert text_clean ->', CLEAN_OUT.name, clean_variant.shape)\n",
    "print('Severity Mapping Export:', severity_to_id_export)\n",
    "print('Hinweis: Für zusätzliche Lemma-Varianten folgt nächste Zelle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162f93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert text_raw_lemma -> cves_processed_text_raw_lemma.csv (53855, 4)\n",
      "Gespeichert text_clean_lemma -> cves_processed_text_clean_lemma.csv (53855, 4)\n"
     ]
    }
   ],
   "source": [
    "# 8a. Zusätzliche Varianten mit Lemmatization (raw & clean)\n",
    "# Diese Zelle erzeugt optionale Dateien mit Lemmatization für beide Pipelines.\n",
    "# Sie nutzt spaCy falls verfügbar; sonst wird übersprungen.\n",
    "LEMMA_RAW_OUT = OUTPUT_DIR / 'cves_processed_text_raw_lemma.csv'\n",
    "LEMMA_CLEAN_OUT = OUTPUT_DIR / 'cves_processed_text_clean_lemma.csv'\n",
    "\n",
    "if nlp is None:\n",
    "    print('spaCy Modell nicht geladen -> Lemma-Varianten werden übersprungen.')\n",
    "else:\n",
    "    def lemmatize_text_series(series):\n",
    "        docs = list(nlp.pipe(series.astype(str).tolist(), batch_size=200, disable=['parser','ner']))\n",
    "        return [' '.join(tok.lemma_ for tok in doc if tok.lemma_) for doc in docs]\n",
    "\n",
    "    # Raw Lemma\n",
    "    raw_variant_lemma = raw_variant.copy()\n",
    "    raw_variant_lemma['description_clean'] = lemmatize_text_series(raw_variant_lemma['description_clean'])\n",
    "    raw_variant_lemma.to_csv(LEMMA_RAW_OUT, index=False)\n",
    "\n",
    "    # Clean Lemma\n",
    "    clean_variant_lemma = clean_variant.copy()\n",
    "    clean_variant_lemma['description_clean'] = lemmatize_text_series(clean_variant_lemma['description_clean'])\n",
    "    clean_variant_lemma.to_csv(LEMMA_CLEAN_OUT, index=False)\n",
    "\n",
    "    print('Gespeichert text_raw_lemma ->', LEMMA_RAW_OUT.name, raw_variant_lemma.shape)\n",
    "    print('Gespeichert text_clean_lemma ->', LEMMA_CLEAN_OUT.name, clean_variant_lemma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15780379",
   "metadata": {},
   "source": [
    "## 9. Token Top-Statistiken Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW VARIANTE: Top Tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>doc_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>109805</td>\n",
       "      <td>38753</td>\n",
       "      <td>71.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>92304</td>\n",
       "      <td>44243</td>\n",
       "      <td>82.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>73463</td>\n",
       "      <td>41095</td>\n",
       "      <td>76.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>70055</td>\n",
       "      <td>36963</td>\n",
       "      <td>68.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>55980</td>\n",
       "      <td>26998</td>\n",
       "      <td>50.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>46210</td>\n",
       "      <td>25370</td>\n",
       "      <td>47.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>an</td>\n",
       "      <td>38119</td>\n",
       "      <td>24251</td>\n",
       "      <td>45.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vulnerability</td>\n",
       "      <td>37043</td>\n",
       "      <td>21678</td>\n",
       "      <td>40.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>27847</td>\n",
       "      <td>18418</td>\n",
       "      <td>34.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>before</td>\n",
       "      <td>25509</td>\n",
       "      <td>13200</td>\n",
       "      <td>24.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token  token_freq  doc_freq  doc_pct\n",
       "0            the      109805     38753    71.96\n",
       "1             to       92304     44243    82.15\n",
       "2             in       73463     41095    76.31\n",
       "3              a       70055     36963    68.63\n",
       "4             of       55980     26998    50.13\n",
       "5            and       46210     25370    47.11\n",
       "6             an       38119     24251    45.03\n",
       "7  vulnerability       37043     21678    40.25\n",
       "8           this       27847     18418    34.20\n",
       "9         before       25509     13200    24.51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN VARIANTE: Top Tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>doc_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vulnerability</td>\n",
       "      <td>70406</td>\n",
       "      <td>25294</td>\n",
       "      <td>46.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attacker</td>\n",
       "      <td>36860</td>\n",
       "      <td>27059</td>\n",
       "      <td>50.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allow</td>\n",
       "      <td>33085</td>\n",
       "      <td>30609</td>\n",
       "      <td>56.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common</td>\n",
       "      <td>23233</td>\n",
       "      <td>5083</td>\n",
       "      <td>9.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>21888</td>\n",
       "      <td>16339</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>remote</td>\n",
       "      <td>19881</td>\n",
       "      <td>17736</td>\n",
       "      <td>32.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open</td>\n",
       "      <td>19173</td>\n",
       "      <td>14327</td>\n",
       "      <td>26.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>code</td>\n",
       "      <td>18271</td>\n",
       "      <td>12871</td>\n",
       "      <td>23.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>via</td>\n",
       "      <td>17491</td>\n",
       "      <td>17242</td>\n",
       "      <td>32.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>service</td>\n",
       "      <td>17065</td>\n",
       "      <td>11503</td>\n",
       "      <td>21.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token  token_freq  doc_freq  doc_pct\n",
       "0  vulnerability       70406     25294    46.97\n",
       "1       attacker       36860     27059    50.24\n",
       "2          allow       33085     30609    56.84\n",
       "3         common       23233      5083     9.44\n",
       "4           user       21888     16339    30.34\n",
       "5         remote       19881     17736    32.93\n",
       "6           open       19173     14327    26.60\n",
       "7           code       18271     12871    23.90\n",
       "8            via       17491     17242    32.02\n",
       "9        service       17065     11503    21.36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemeinsame Tokens (DocFreq Delta):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_freq_raw</th>\n",
       "      <th>doc_freq_raw</th>\n",
       "      <th>doc_pct_raw</th>\n",
       "      <th>token_freq_clean</th>\n",
       "      <th>doc_freq_clean</th>\n",
       "      <th>doc_pct_clean</th>\n",
       "      <th>doc_pct_delta_clean_minus_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vulnerability</td>\n",
       "      <td>37043</td>\n",
       "      <td>21678</td>\n",
       "      <td>40.25</td>\n",
       "      <td>70406</td>\n",
       "      <td>25294</td>\n",
       "      <td>46.97</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token  token_freq_raw  doc_freq_raw  doc_pct_raw  token_freq_clean  \\\n",
       "0  vulnerability           37043         21678        40.25             70406   \n",
       "\n",
       "   doc_freq_clean  doc_pct_clean  doc_pct_delta_clean_minus_raw  \n",
       "0           25294          46.97                           6.72  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VAR_DIR = Path('../data/processed/')\n",
    "FILES = {\n",
    "    'raw': VAR_DIR / 'cves_processed_text_raw.csv',\n",
    "    'clean': VAR_DIR / 'cves_processed_text_clean.csv',\n",
    "    'raw_lemma': VAR_DIR / 'cves_processed_text_raw_lemma.csv',\n",
    "    'clean_lemma': VAR_DIR / 'cves_processed_text_clean_lemma.csv'\n",
    "}\n",
    "\n",
    "TOP_N = 10\n",
    "from collections import Counter\n",
    "\n",
    "def top_tokens(series, top_n):\n",
    "    texts = series.astype(str)\n",
    "    N = len(texts)\n",
    "    tf_counter = Counter()\n",
    "    df_counter = Counter()\n",
    "    for row in texts:\n",
    "        toks = row.split()\n",
    "        if not toks:\n",
    "            continue\n",
    "        tf_counter.update(toks)\n",
    "        df_counter.update(set(toks))\n",
    "    mc = tf_counter.most_common(top_n)\n",
    "    rows = []\n",
    "    for tok, tf in mc:\n",
    "        dfreq = df_counter.get(tok, 0)\n",
    "        rows.append((tok, tf, dfreq, round(dfreq / N * 100, 2)))\n",
    "    return pd.DataFrame(rows, columns=['token','token_freq','doc_freq','doc_pct'])\n",
    "\n",
    "available = {k:p for k,p in FILES.items() if p.exists()}\n",
    "print('Gefundene Varianten:', {k: p.name for k,p in available.items()})\n",
    "variant_dfs = {}\n",
    "for name, path in available.items():\n",
    "    df_var = pd.read_csv(path)\n",
    "    if 'description_clean' not in df_var.columns:\n",
    "        print(f'WARN: description_clean fehlt in {path.name}, übersprungen')\n",
    "        continue\n",
    "    variant_dfs[name] = df_var\n",
    "\n",
    "stats = {}\n",
    "for name, dfv in variant_dfs.items():\n",
    "    stats[name] = top_tokens(dfv['description_clean'], TOP_N)\n",
    "    print(f'\\nVariante: {name} (N={len(dfv)}) Top Tokens:')\n",
    "    display(stats[name])\n",
    "\n",
    "# Vergleich zwischen Paaren (raw vs clean) und (raw_lemma vs clean_lemma) falls vorhanden\n",
    "from itertools import combinations\n",
    "pair_specs = [('raw','clean'), ('raw_lemma','clean_lemma')]\n",
    "for a,b in pair_specs:\n",
    "    if a in stats and b in stats:\n",
    "        merged = pd.merge(stats[a], stats[b], on='token', how='inner', suffixes=(f'_{a}', f'_{b}'))\n",
    "        if not merged.empty:\n",
    "            merged[f'doc_pct_delta_{b}_minus_{a}'] = merged[f'doc_pct_{b}'] - merged[f'doc_pct_{a}']\n",
    "            print(f'\\nGemeinsame Tokens Delta ({a} -> {b}):')\n",
    "            display(merged.head(20))\n",
    "        else:\n",
    "            print(f'Keine gemeinsamen Top Tokens zwischen {a} und {b}.')\n",
    "\n",
    "# Optional: Schnittmenge aller vorhandenen Varianten\n",
    "if len(stats) > 1:\n",
    "    common = set.intersection(*(set(dfv['token']) for dfv in stats.values())) if all('token' in dfv.columns for dfv in stats.values()) else set()\n",
    "    if common:\n",
    "        print(f'Gemeinsame Tokens in allen Varianten ({len(common)}):', list(sorted(list(common)))[:30])\n",
    "    else:\n",
    "        print('Keine gemeinsamen Tokens in allen Varianten (Top-N Ebene).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
