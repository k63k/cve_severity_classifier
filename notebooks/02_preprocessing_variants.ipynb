{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54ba17d",
   "metadata": {},
   "source": [
    "# Notebook 02: Preprocessing (verschiedene Varianten)\n",
    "\n",
    "Konsolidierte Erstellung exakt **zweier** Preprocessing-Varianten für die Modelle. Fokus: klarer, kontrollierter Vergleich einer minimalen vs. einer angereicherten Pipeline.\n",
    "\n",
    "1. **raw_minimal** – nur Lowercasing + Whitespace-Normalisierung (keine Acronym-Expansion, keine Stopwords, keine Lemmatization). Bewahrt originale Tokenformen (Rauschen inklusive) für eine nüchterne Basis.\n",
    "2. **clean_lemma_stop** – erweiterte Pipeline: Normalisierung (HTML/Punctuation/Digits), Acronym-Expansion sicherheitsrelevanter Kürzel, Stopword-Entfernung, optional Lemmatization (abhängig von `USE_LEMMATIZATION` + spaCy Modell).\n",
    "\n",
    "Ausgabe CSV-Dateien (Ordner `data/processed/`):\n",
    "- `cves_processed_text_raw.csv`  ← entspricht Variante `raw_minimal`\n",
    "- `cves_processed_text_clean.csv` ← entspricht Variante `clean_lemma_stop`\n",
    "\n",
    "Optionale Zusatz-Exports (falls spaCy verfügbar & Lemmatization aktiv):\n",
    "- `cves_processed_text_raw_lemma.csv`\n",
    "- `cves_processed_text_clean_lemma.csv`\n",
    "\n",
    "Ziel: Minimal vs. angereichert isoliert bewerten ohne Variantenexplosion. Inhalte stammen aus dem Legacy-Notebook; nur Umbenennung & Klarstellung der Variantendefinitionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8d567",
   "metadata": {},
   "source": [
    "## 1. Imports & Grundeinstellungen\n",
    "\n",
    "Initialisiert Bibliotheken, Stopword-Liste und optional spaCy für Lemmatization. Wählt eine vorhandene Roh-CSV (Prioritätsliste) und setzt reproduzierbare Flags (`USE_LEMMATIZATION`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccda3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "USE_LEMMATIZATION = True  # optional (clean Variante)\n",
    "\n",
    "# Stopwords sicherstellen\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# spaCy optional\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser','ner']) if USE_LEMMATIZATION else None\n",
    "    except OSError:\n",
    "        print('Hinweis: en_core_web_sm nicht installiert')\n",
    "        nlp = None\n",
    "except ImportError:\n",
    "    if USE_LEMMATIZATION:\n",
    "        print('spaCy nicht installiert – Lemmatisierung deaktiviert')\n",
    "    nlp = None\n",
    "\n",
    "DATA_DIR = Path('..') / 'data' / 'raw'\n",
    "DEFAULT_CSV_LIST = ['cves_v40.csv','cves_v31.csv','cves_v30.csv','cves_v2.csv']\n",
    "CSV_SOURCE = 'cves_v30.csv'\n",
    "\n",
    "if CSV_SOURCE is None:\n",
    "    candidates = DEFAULT_CSV_LIST\n",
    "elif isinstance(CSV_SOURCE, str):\n",
    "    candidates = [CSV_SOURCE]\n",
    "elif isinstance(CSV_SOURCE, (list, tuple)):\n",
    "    candidates = list(CSV_SOURCE)\n",
    "else:\n",
    "    raise ValueError('CSV_SOURCE muss None, str oder Liste sein')\n",
    "CSV_PATH = next((DATA_DIR / c for c in candidates if (DATA_DIR / c).exists()), None)\n",
    "print({'candidates': candidates, 'CSV_PATH': str(CSV_PATH)})\n",
    "assert CSV_PATH is not None, 'Keine CVE CSV gefunden.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371eb268",
   "metadata": {},
   "source": [
    "## 2. Laden der Rohdaten\n",
    "\n",
    "Liest die erste existierende CVE-CSV aus einer Prioritätsliste (`DEFAULT_CSV_LIST`). Nutzt nur relevante Spalten (`cve_id`, `severity`, `description`) und verwirft Zeilen ohne Beschreibung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLS = ['cve_id','severity','description']\n",
    "df = pd.read_csv(CSV_PATH, usecols=USE_COLS)\n",
    "print('Zeilen eingelesen:', len(df))\n",
    "print(df.head(3)[USE_COLS])\n",
    "df = df.dropna(subset=['description'])\n",
    "print('Nach Drop NA:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aed577",
   "metadata": {},
   "source": [
    "## 3. Acronym Expansion Mapping\n",
    "\n",
    "Ersetzt sicherheitsrelevante Kürzel (z.B. `rce`, `xss`, `lpe`) durch semantisch reichere Phrasen bevor Stopwords/Lemmas angewendet werden. Ziel: Informationsverdichtung (mehr Tokens mit Bedeutung) und Vereinheitlichung unterschiedlicher Schreibweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c437a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACRONYM_MAP = {\n",
    "    # Impact-/Exploit-Typen\n",
    "    'rce': 'remote code execution',\n",
    "    'codeexec': 'remote code execution',\n",
    "    'eop': 'elevation of privilege',\n",
    "    'privesc': 'privilege escalation',\n",
    "    'pe': 'privilege escalation',\n",
    "    'lpe': 'local privilege escalation',\n",
    "    'uep': 'unauthorized privilege escalation',\n",
    "    'dos': 'denial of service',\n",
    "    'crash': 'denial of service',\n",
    "    'ddos': 'distributed denial of service',\n",
    "    'infoleak': 'information disclosure',\n",
    "    'id': 'information disclosure',\n",
    "    'leak': 'information disclosure',\n",
    "\n",
    "    # Speicher-/Memory-Corruption\n",
    "    'bof': 'buffer overflow',\n",
    "    'bo': 'buffer overflow',\n",
    "    'heapof': 'heap overflow',\n",
    "    'heapovf': 'heap overflow',\n",
    "    'heapoverflow': 'heap overflow',\n",
    "    'stackof': 'stack overflow',\n",
    "    'stackoverflow': 'stack overflow',\n",
    "    'oob': 'out of bounds access',\n",
    "    'oobr': 'out of bounds read',\n",
    "    'oobw': 'out of bounds write',\n",
    "    'iob': 'index out of bounds',\n",
    "    'uaf': 'use after free',\n",
    "    'df': 'double free',\n",
    "    'npd': 'null pointer dereference',\n",
    "    'tc': 'type confusion',\n",
    "    'typeconf': 'type confusion',\n",
    "    'fmtstr': 'format string vulnerability',\n",
    "    'race': 'race condition',\n",
    "    'toctou': 'time of check to time of use',\n",
    "    'intof': 'integer overflow',\n",
    "    'intuf': 'integer underflow',\n",
    "    'sigrop': 'signal oriented programming',\n",
    "\n",
    "    # Web-/AppSec-Klassen\n",
    "    'xss': 'cross site scripting',\n",
    "    'xxs': 'cross site scripting',\n",
    "    'uxss': 'universal cross site scripting',\n",
    "    'csrf': 'cross site request forgery',\n",
    "    'xsrf': 'cross site request forgery',\n",
    "    'ssrf': 'server side request forgery',\n",
    "    'sqli': 'sql injection',\n",
    "    'nosqli': 'nosql injection',\n",
    "    'ldapi': 'ldap injection',\n",
    "    'ognl': 'ognl expression injection',\n",
    "    'cmdi': 'command injection',\n",
    "    'rce-inj': 'command injection',\n",
    "    'ssti': 'server side template injection',\n",
    "    'xssi': 'cross site script inclusion',\n",
    "    'xxe': 'xml external entity',\n",
    "    'xee': 'xml external entity',\n",
    "    'lfi': 'local file inclusion',\n",
    "    'rfi': 'remote file inclusion',\n",
    "    'idor': 'insecure direct object reference',\n",
    "    'bidor': 'blind insecure direct object reference',\n",
    "    'dirtrav': 'directory traversal',\n",
    "    'pathtrav': 'directory traversal',\n",
    "    'openredirect': 'open redirect',\n",
    "    'or': 'open redirect',\n",
    "    'crlfi': 'crlf injection',\n",
    "    'hpp': 'http parameter pollution',\n",
    "    'hhi': 'host header injection',\n",
    "    'reqsmuggle': 'http request smuggling',\n",
    "    'hrs': 'http request smuggling',\n",
    "    'reqsplit': 'http response splitting',\n",
    "    'cachepoison': 'web cache poisoning',\n",
    "    'deser': 'insecure deserialization',\n",
    "    'insecdeser': 'insecure deserialization',\n",
    "    'proto_pollution': 'prototype pollution',\n",
    "    'fileupload': 'insecure file upload',\n",
    "    'zipSlip': 'zip slip path traversal',\n",
    "\n",
    "    # AuthN/AuthZ/Session\n",
    "    'ato': 'account takeover',\n",
    "    '2fa': 'two factor authentication',\n",
    "    'mfa': 'multi factor authentication',\n",
    "    'sso': 'single sign on',\n",
    "    'saml': 'security assertion markup language',\n",
    "    'oauth': 'oauth',\n",
    "    'oidc': 'openid connect',\n",
    "    'jwt': 'json web token',\n",
    "    'jwts': 'json web token',\n",
    "    'jwk': 'json web key',\n",
    "    'jwks': 'json web key set',\n",
    "    'pkce': 'proof key for code exchange',\n",
    "    'bruteforce': 'credential brute force',\n",
    "    'credstuff': 'credential stuffing',\n",
    "\n",
    "    # Protokolle/HTTP-Sonderfälle\n",
    "    'wsx': 'websocket hijacking',\n",
    "    'dnsrb': 'dns rebinding',\n",
    "    'ssrf-blind': 'blind server side request forgery',\n",
    "\n",
    "    # Plattform-/Mitigations-/Exploitation-Techniques\n",
    "    'aslr': 'address space layout randomization',\n",
    "    'kaslr': 'kernel address space layout randomization',\n",
    "    'dep': 'data execution prevention',\n",
    "    'nx': 'no execute',\n",
    "    'rop': 'return oriented programming',\n",
    "    'jop': 'jump oriented programming',\n",
    "    'cfi': 'control flow integrity',\n",
    "    'cet': 'control-flow enforcement technology',\n",
    "    'pie': 'position independent executable',\n",
    "    'relro': 'relocation read-only',\n",
    "    'pac': 'pointer authentication',\n",
    "    'sandbox': 'process sandboxing',\n",
    "    'seccomp': 'secure computing mode',\n",
    "    'fortify': 'fortify source hardening',\n",
    "\n",
    "    # Cloud/Config/Secrets\n",
    "    'misconfig': 'security misconfiguration',\n",
    "    'openbucket': 'public cloud storage misconfiguration',\n",
    "    's3public': 'public cloud storage misconfiguration',\n",
    "    'imds': 'cloud instance metadata service exposure',\n",
    "    'k8srbac': 'kubernetes rbac misconfiguration',\n",
    "    'secretleak': 'secrets exposure',\n",
    "    'hardcodedcred': 'hardcoded credentials',\n",
    "\n",
    "    # Kataloge/Scoring/Taxonomien\n",
    "    'cve': 'common vulnerabilities and exposures',\n",
    "    'cwe': 'common weakness enumeration',\n",
    "    'cvss': 'common vulnerability scoring system',\n",
    "    'epss': 'exploit prediction scoring system',\n",
    "    'cpe': 'common platform enumeration',\n",
    "\n",
    "    # Sonstiges/Meta\n",
    "    'poc': 'proof of concept',\n",
    "    'exploit': 'exploit',\n",
    "    '0day': 'zero day',\n",
    "    'zeroday': 'zero day',\n",
    "    'nday': 'n day',\n",
    "}\n",
    "ACRONYM_REGEX = re.compile(r'\\b(' + '|'.join(map(re.escape, ACRONYM_MAP.keys())) + r')\\b', re.IGNORECASE)\n",
    "def expand_acronyms(text: str) -> str:\n",
    "    def repl(m):\n",
    "        return ACRONYM_MAP.get(m.group(1).lower(), m.group(1))\n",
    "    return ACRONYM_REGEX.sub(repl, text)\n",
    "print(expand_acronyms('RCE via SSRF causes LPE + XSS & DoS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd9309",
   "metadata": {},
   "source": [
    "## 4. Preprocessing-Schritte für Variante `clean_lemma_stop`\n",
    "\n",
    "Pipeline-Komponenten (in Reihenfolge):\n",
    "1. Lowercasing & HTML-Unescape\n",
    "2. Entfernen von HTML-Tags, Punctuation, reinen Ziffern\n",
    "3. Normalisierung (nur a–z, 0–9, Single-Space)\n",
    "4. Acronym Expansion (ersetzt sicherheitsrelevante Kürzel)\n",
    "5. Tokenisierung per Space-Split\n",
    "6. Stopword-Entfernung (`nltk` english)\n",
    "7. (Optional) Lemmatization via spaCy, falls Modell geladen\n",
    "\n",
    "Ergebnisfeld: `description_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_REGEX = re.compile(r\"[\\\"'`´’“”‘()\\[\\]{}<>=:;,+*/\\\\|~^]\")\n",
    "MULTI_WS = re.compile(r'\\s{2,}')\n",
    "HTML_TAG = re.compile(r'<[^>]+>')\n",
    "DIGITS = re.compile(r'\\b\\d+\\b')\n",
    "NON_ALPHANUM = re.compile(r'[^a-z0-9 ]')\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = html.unescape(text)\n",
    "    text = HTML_TAG.sub(' ', text)\n",
    "    text = text.replace('\\t',' ').replace('\\n',' ')\n",
    "    text = PUNCT_REGEX.sub(' ', text)\n",
    "    text = DIGITS.sub(' ', text)\n",
    "    text = NON_ALPHANUM.sub(' ', text)\n",
    "    text = MULTI_WS.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    if not USE_LEMMATIZATION or nlp is None:\n",
    "        return tokens\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return [tok.lemma_ for tok in doc]\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = normalize_basic(text)\n",
    "    text = expand_acronyms(text)\n",
    "    tokens = text.split(' ')\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(preprocess_text('This RCE issue allows Remote Code Execution and XSS via SQLi.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc7ccb",
   "metadata": {},
   "source": [
    "## 5. Anwendung der Pipeline (`clean_lemma_stop`)\n",
    "\n",
    "Batchweise Verarbeitung (optional Begrenzung über `LIMIT_ROWS`) und – falls verfügbar – spaCy Lemmatization im Pipe-Modus zur Beschleunigung. Zwischenschritte: normalize → expand acronyms → tokenize → stopwords → (lemmatize) → join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beschleunigte Anwendung der Pipeline (clean Variante) mit Batch-Verarbeitung\n",
    "import time\n",
    "START_TIME = time.time()\n",
    "\n",
    "# Optionale Begrenzung für schnellere Iterationen\n",
    "LIMIT_ROWS = None  # z.B. 5000 für schnellen Test\n",
    "BATCH_SIZE_SPACY = 400\n",
    "\n",
    "work_df = df if LIMIT_ROWS is None else df.head(LIMIT_ROWS).copy()\n",
    "texts = work_df['description'].astype(str).tolist()\n",
    "\n",
    "normalized = [normalize_basic(t) for t in texts]\n",
    "expanded = [expand_acronyms(t) for t in normalized]\n",
    "# Tokenisierung simpel per split (später für spaCy Lemmas ersetzt)\n",
    "token_lists = [t.split(' ') for t in expanded]\n",
    "\n",
    "# Stopwords entfernen\n",
    "filtered_tokens = [[tok for tok in toks if tok and tok not in STOPWORDS and len(tok) > 1] for toks in token_lists]\n",
    "\n",
    "if USE_LEMMATIZATION and nlp is not None:\n",
    "    # Batch Lemmatization nur auf gefilterten Tokens\n",
    "    joined = [' '.join(toks) for toks in filtered_tokens]\n",
    "    lemmas = []\n",
    "    for doc in nlp.pipe(joined, batch_size=BATCH_SIZE_SPACY, disable=['parser','ner']):\n",
    "        lemmas.append(' '.join(tok.lemma_ for tok in doc if tok.lemma_))\n",
    "    final_texts = lemmas\n",
    "else:\n",
    "    final_texts = [' '.join(toks) for toks in filtered_tokens]\n",
    "\n",
    "work_df['description_clean'] = final_texts\n",
    "\n",
    "elapsed = time.time() - START_TIME\n",
    "print(f'Fertig: {len(work_df)} Zeilen preprocesset in {elapsed:.2f}s (Lemmatization={USE_LEMMATIZATION and nlp is not None})')\n",
    "print(work_df[['description','description_clean']].head(5))\n",
    "print('Ø Tokens nach Preprocessing:', round(work_df['description_clean'].str.split().apply(len).mean(),2))\n",
    "\n",
    "# In Haupt-df zurückschreiben\n",
    "if LIMIT_ROWS is None:\n",
    "    df = work_df\n",
    "else:\n",
    "    # Nur subset ersetzt; Hinweis ausgeben\n",
    "    df.loc[work_df.index, 'description_clean'] = work_df['description_clean']\n",
    "    print('WARN: LIMIT_ROWS aktiv, nur Teilmenge verarbeitet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57bdb42",
   "metadata": {},
   "source": [
    "## 6. Severity Encoding & Filter\n",
    "\n",
    "Normalisiert Labels (Lowercase), filtert unbekannte Werte und mappt die Ordnung `low < medium < high < critical` auf Integer-IDs (`severity_id`). Entfernt inkonsistente Zeilen für saubere Klassifikationsziele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_SEVERITIES = ['low','medium','high','critical']\n",
    "sev_norm = df['severity'].astype(str).str.lower()\n",
    "unknown = sorted(set(sev_norm) - set(ORDERED_SEVERITIES))\n",
    "if unknown:\n",
    "    print('Verwerfe unbekannte Severity Labels:', unknown)\n",
    "    before = len(df)\n",
    "    df = df[sev_norm.isin(ORDERED_SEVERITIES)].copy()\n",
    "    sev_norm = df['severity'].astype(str).str.lower()\n",
    "    print(f'Entfernt {before - len(df)} Zeilen aufgrund unbekannter Labels.')\n",
    "severity_to_id = {s:i for i,s in enumerate(ORDERED_SEVERITIES)}\n",
    "severity_ids = sev_norm.map(severity_to_id)\n",
    "assert not severity_ids.isna().any(), 'NaN im Severity Mapping nach Filter'\n",
    "df['severity_id'] = severity_ids.astype(int)\n",
    "print('Severity Mapping:', severity_to_id)\n",
    "print('Verteilung:', df['severity_id'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3ce63",
   "metadata": {},
   "source": [
    "## 7. Basisstatistik\n",
    "\n",
    "Schnelle Qualitätskontrolle: Anzahl Zeilen, einzigartige CVE-IDs sowie durchschnittliche Token-Länge nach Preprocessing (`description_clean`). Dient zur Plausibilisierung des Token-Reduktions-Effekts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739678af",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    'n_rows': int(len(df)),\n",
    "    'n_unique_cve': int(df['cve_id'].nunique()) if 'cve_id' in df.columns else None,\n",
    "    'avg_tokens_post': float(df['description_clean'].str.split().apply(len).mean()),\n",
    "}\n",
    "print(df[['description_clean','severity']].sample(min(3, len(df))))\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483909a",
   "metadata": {},
   "source": [
    "## 8. Varianten-Export\n",
    "\n",
    "Schreibt die beiden Kernvarianten:\n",
    "- `cves_processed_text_raw.csv` → Variante `raw_minimal`\n",
    "- `cves_processed_text_clean.csv` → Variante `clean_lemma_stop`\n",
    "\n",
    "Zusätzlich (falls Lemmatization aktiv + spaCy vorhanden):\n",
    "- `cves_processed_text_raw_lemma.csv`\n",
    "- `cves_processed_text_clean_lemma.csv`\n",
    "\n",
    "Alle Dateien enthalten: `cve_id`, `severity`, `severity_id`, `description_clean`. Einheitliche Schema erleichtert spätere Modell-Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_OUT = OUTPUT_DIR / 'cves_processed_text_raw.csv'\n",
    "CLEAN_OUT = OUTPUT_DIR / 'cves_processed_text_clean.csv'\n",
    "base_df = df[['cve_id','severity','severity_id','description_clean']].copy()\n",
    "\n",
    "# Raw Variante erneut einlesen (minimales Cleaning + gleiche Severity-Filterlogik)\n",
    "raw_src = pd.read_csv(CSV_PATH, usecols=['cve_id','severity','description']).dropna()\n",
    "raw_src['severity'] = raw_src['severity'].astype(str)\n",
    "raw_src_norm = raw_src['severity'].str.lower()\n",
    "unknown_raw = sorted(set(raw_src_norm) - set(ORDERED_SEVERITIES))\n",
    "if unknown_raw:\n",
    "    before = len(raw_src)\n",
    "    raw_src = raw_src[raw_src_norm.isin(ORDERED_SEVERITIES)].copy()\n",
    "    raw_src_norm = raw_src['severity'].str.lower()\n",
    "    print(f'Raw: entferne {before - len(raw_src)} Zeilen wegen unbekannter Severity Labels: {unknown_raw}')\n",
    "severity_to_id_export = {s:i for i,s in enumerate(ORDERED_SEVERITIES)}\n",
    "raw_src['severity_id'] = raw_src_norm.map(severity_to_id_export)\n",
    "\n",
    "def minimal_clean(t: str) -> str:\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip()\n",
    "raw_src['description_clean'] = raw_src['description'].apply(minimal_clean)\n",
    "raw_variant = raw_src[['cve_id','severity','severity_id','description_clean']].copy()\n",
    "raw_variant.to_csv(RAW_OUT, index=False)\n",
    "clean_variant = base_df.copy()\n",
    "clean_variant.to_csv(CLEAN_OUT, index=False)\n",
    "print('Gespeichert text_raw ->', RAW_OUT.name, raw_variant.shape)\n",
    "print('Gespeichert text_clean ->', CLEAN_OUT.name, clean_variant.shape)\n",
    "print('Severity Mapping Export:', severity_to_id_export)\n",
    "print('Hinweis: Für zusätzliche Lemma-Varianten folgt nächste Zelle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8a. Zusätzliche Varianten mit Lemmatization (raw & clean)\n",
    "# Diese Zelle erzeugt optionale Dateien mit Lemmatization für beide Pipelines.\n",
    "# Sie nutzt spaCy falls verfügbar; sonst wird übersprungen.\n",
    "LEMMA_RAW_OUT = OUTPUT_DIR / 'cves_processed_text_raw_lemma.csv'\n",
    "LEMMA_CLEAN_OUT = OUTPUT_DIR / 'cves_processed_text_clean_lemma.csv'\n",
    "\n",
    "if nlp is None:\n",
    "    print('spaCy Modell nicht geladen -> Lemma-Varianten werden übersprungen.')\n",
    "else:\n",
    "    def lemmatize_text_series(series):\n",
    "        docs = list(nlp.pipe(series.astype(str).tolist(), batch_size=200, disable=['parser','ner']))\n",
    "        return [' '.join(tok.lemma_ for tok in doc if tok.lemma_) for doc in docs]\n",
    "\n",
    "    # Raw Lemma\n",
    "    raw_variant_lemma = raw_variant.copy()\n",
    "    raw_variant_lemma['description_clean'] = lemmatize_text_series(raw_variant_lemma['description_clean'])\n",
    "    raw_variant_lemma.to_csv(LEMMA_RAW_OUT, index=False)\n",
    "\n",
    "    # Clean Lemma\n",
    "    clean_variant_lemma = clean_variant.copy()\n",
    "    clean_variant_lemma['description_clean'] = lemmatize_text_series(clean_variant_lemma['description_clean'])\n",
    "    clean_variant_lemma.to_csv(LEMMA_CLEAN_OUT, index=False)\n",
    "\n",
    "    print('Gespeichert text_raw_lemma ->', LEMMA_RAW_OUT.name, raw_variant_lemma.shape)\n",
    "    print('Gespeichert text_clean_lemma ->', LEMMA_CLEAN_OUT.name, clean_variant_lemma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15780379",
   "metadata": {},
   "source": [
    "## 9. Token-Häufigkeiten & Variantenvergleich\n",
    "\n",
    "Ermittelt für jede vorhandene Variante die Top-N Tokens (Term Frequency + Dokumentfrequenz + Prozentanteil). Vergleicht Paarweise (`raw` vs `clean`, `raw_lemma` vs `clean_lemma`) zur Beobachtung von:\n",
    "- Reduktion häufiger Funktions-/Stoppwörter\n",
    "- Effekt von Acronym Expansion (mehr mehrwortige Phrasen)\n",
    "- Einfluss der Lemmatization auf Token-Vielfalt\n",
    "\n",
    "Dient der qualitativen Validierung, dass `clean_lemma_stop` tatsächlich Informationsdichte erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_DIR = Path('../data/processed/')\n",
    "FILES = {\n",
    "    'raw': VAR_DIR / 'cves_processed_text_raw.csv',\n",
    "    'clean': VAR_DIR / 'cves_processed_text_clean.csv',\n",
    "    'raw_lemma': VAR_DIR / 'cves_processed_text_raw_lemma.csv',\n",
    "    'clean_lemma': VAR_DIR / 'cves_processed_text_clean_lemma.csv'\n",
    "}\n",
    "\n",
    "TOP_N = 10\n",
    "from collections import Counter\n",
    "\n",
    "def top_tokens(series, top_n):\n",
    "    texts = series.astype(str)\n",
    "    N = len(texts)\n",
    "    tf_counter = Counter()\n",
    "    df_counter = Counter()\n",
    "    for row in texts:\n",
    "        toks = row.split()\n",
    "        if not toks:\n",
    "            continue\n",
    "        tf_counter.update(toks)\n",
    "        df_counter.update(set(toks))\n",
    "    mc = tf_counter.most_common(top_n)\n",
    "    rows = []\n",
    "    for tok, tf in mc:\n",
    "        dfreq = df_counter.get(tok, 0)\n",
    "        rows.append((tok, tf, dfreq, round(dfreq / N * 100, 2)))\n",
    "    return pd.DataFrame(rows, columns=['token','token_freq','doc_freq','doc_pct'])\n",
    "\n",
    "available = {k:p for k,p in FILES.items() if p.exists()}\n",
    "print('Gefundene Varianten:', {k: p.name for k,p in available.items()})\n",
    "variant_dfs = {}\n",
    "for name, path in available.items():\n",
    "    df_var = pd.read_csv(path)\n",
    "    if 'description_clean' not in df_var.columns:\n",
    "        print(f'WARN: description_clean fehlt in {path.name}, übersprungen')\n",
    "        continue\n",
    "    variant_dfs[name] = df_var\n",
    "\n",
    "stats = {}\n",
    "for name, dfv in variant_dfs.items():\n",
    "    stats[name] = top_tokens(dfv['description_clean'], TOP_N)\n",
    "    print(f'\\nVariante: {name} (N={len(dfv)}) Top Tokens:')\n",
    "    display(stats[name])\n",
    "\n",
    "# Vergleich zwischen Paaren (raw vs clean) und (raw_lemma vs clean_lemma) falls vorhanden\n",
    "from itertools import combinations\n",
    "pair_specs = [('raw','clean'), ('raw_lemma','clean_lemma')]\n",
    "for a,b in pair_specs:\n",
    "    if a in stats and b in stats:\n",
    "        merged = pd.merge(stats[a], stats[b], on='token', how='inner', suffixes=(f'_{a}', f'_{b}'))\n",
    "        if not merged.empty:\n",
    "            merged[f'doc_pct_delta_{b}_minus_{a}'] = merged[f'doc_pct_{b}'] - merged[f'doc_pct_{a}']\n",
    "            print(f'\\nGemeinsame Tokens Delta ({a} -> {b}):')\n",
    "            display(merged.head(20))\n",
    "        else:\n",
    "            print(f'Keine gemeinsamen Top Tokens zwischen {a} und {b}.')\n",
    "\n",
    "# Optional: Schnittmenge aller vorhandenen Varianten\n",
    "if len(stats) > 1:\n",
    "    common = set.intersection(*(set(dfv['token']) for dfv in stats.values())) if all('token' in dfv.columns for dfv in stats.values()) else set()\n",
    "    if common:\n",
    "        print(f'Gemeinsame Tokens in allen Varianten ({len(common)}):', list(sorted(list(common)))[:30])\n",
    "    else:\n",
    "        print('Keine gemeinsamen Tokens in allen Varianten (Top-N Ebene).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
