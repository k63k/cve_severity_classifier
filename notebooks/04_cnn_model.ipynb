{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb16433",
   "metadata": {},
   "source": [
    "# Notebook 04: CNN CVE Severity Classification (Refactor v2)\n",
    "\n",
    "Reiner Refactor der bestehenden CNN-Experimente (PyTorch) zur Reduktion von Redundanz. Keine neuen Features, nur Konsolidierung:\n",
    "- Zusammengefasste Imports & Konfiguration\n",
    "- Gemeinsame Hilfsfunktionen (Vokabular, Dataset, Training)\n",
    "- Baseline Run + Param-Sweep wie original\n",
    "- Interpretierbarkeit (Filter n-Grams) beibehalten\n",
    "- Persistenz: `results/metrics_cnn.csv`, `results/cnn_filter_activation_examples.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a2ca4",
   "metadata": {},
   "source": [
    "## 1. Imports & Globale Konfiguration\n",
    "\n",
    "Sammelt alle Bibliotheken, stellt Seed-Konfiguration und globale Hyperparameter bereit (Vokabular, Embedding-Dimensionen, Filtergrößen, Trainingsparameter). Ergebnisse & Artefakte werden nach `results/` geschrieben.\n",
    "\n",
    "\n",
    "Setzt deterministische Seeds für Python, NumPy und PyTorch (inkl. CUDA falls verfügbar). Hinweis: Volle Deterministik kann Performance kosten, ist hier aber für Vergleichbarkeit wichtiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ce8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated imports & config (placed early so helpers can use them)\n",
    "import os, random, json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "DATA_PROCESSED = Path('data/processed') if Path('data/processed').exists() else Path('..') / 'data' / 'processed'\n",
    "RESULTS_DIR = Path('results') if Path('results').exists() else Path('..') / 'results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LEN = 160\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "PATIENCE = 2\n",
    "LR = 1e-3\n",
    "FILTER_SIZES = [3,4,5]\n",
    "FILTERS_PER_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "EMBED_DIMS = [50, 100, 200]\n",
    "VOCAB_SIZES = [10000, 25000]\n",
    "MIN_FREQS = [1, 2]\n",
    "CLASS_NAMES = ['low','medium','high','critical']\n",
    "CLASS_TO_ID = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "\n",
    "print({'results_dir': str(RESULTS_DIR), 'data_dir': str(DATA_PROCESSED)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73b3c5",
   "metadata": {},
   "source": [
    "## 2. Daten- & Vokabular-Hilfsfunktionen\n",
    "\n",
    "Lädt Varianten, erkennt Text- und Labelspalten heuristisch, Tokenisierung (Whitespace, lowercase) und Vokabularaufbau mit `min_freq` & Begrenzung durch `vocab_size`. Dataset kapselt Padding & Trunkierung (`MAX_LEN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e72028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: load variants, detect columns, vocab, dataset\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "VARIANT_FILES = {\n",
    "    'raw': 'cves_processed_text_raw.csv',\n",
    "    'clean': 'cves_processed_text_clean.csv',\n",
    "    'raw_lemma': 'cves_processed_text_raw_lemma.csv',\n",
    "    'clean_lemma': 'cves_processed_text_clean_lemma.csv'\n",
    "}\n",
    "\n",
    "TEXT_CANDIDATES = ['description_clean','description','text','summary']\n",
    "LABEL_CANDIDATES = ['severity','cvss_severity','baseSeverity','label']\n",
    "\n",
    "def load_variant(name: str) -> pd.DataFrame:\n",
    "    fname = VARIANT_FILES[name]\n",
    "    fp = DATA_PROCESSED / fname\n",
    "    df = pd.read_csv(fp)\n",
    "    return df\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    text_col = None\n",
    "    for c in TEXT_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            text_col = c; break\n",
    "    if text_col is None:\n",
    "        object_cols = [c for c in df.columns if df[c].dtype=='object']\n",
    "        avg_lengths = {c: df[c].astype(str).str.split().str.len().mean() for c in object_cols}\n",
    "        text_col = max(avg_lengths, key=avg_lengths.get)\n",
    "    label_col = None\n",
    "    for c in LABEL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            label_col = c; break\n",
    "    if label_col is None:\n",
    "        raise ValueError('No label column found.')\n",
    "    return text_col, label_col\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def build_vocab(texts, vocab_size: int, min_freq: int):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenize(t))\n",
    "    items = [(tok,freq) for tok,freq in counter.items() if freq >= min_freq]\n",
    "    items.sort(key=lambda x: (-x[1], x[0]))\n",
    "    trimmed = items[:vocab_size-2]\n",
    "    stoi = {tok:i+2 for i,(tok,_) in enumerate(trimmed)}\n",
    "    stoi['<pad>'] = 0\n",
    "    stoi['<unk>'] = 1\n",
    "    return stoi\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def encode(self, text):\n",
    "        toks = tokenize(text)\n",
    "        ids = [self.vocab.get(t,1) for t in toks][:self.max_len]\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [0]*(self.max_len - len(ids))\n",
    "        return ids\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encode(self.texts[idx]), dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226c779",
   "metadata": {},
   "source": [
    "## 3. Modellarchitektur (Multi-Kernel CNN)\n",
    "\n",
    "Embedding + parallele 1D-Convs (Kernelgrößen in `FILTER_SIZES`), ReLU, Global-Max-Pooling, Feature-Konkatenation, Dropout und lineare Klassifikation. Aktivierungen für Interpretierbarkeit werden zwischengespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a81e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition (multi-kernel CNN matching original logic)\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int, filter_sizes, filters_per_size: int, dropout: float, pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=filters_per_size, kernel_size=fs, padding=fs//2)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(filters_per_size * len(filter_sizes), num_classes)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)      # (B, L, E)\n",
    "        emb = emb.transpose(1,2)     # (B, E, L)\n",
    "        conv_outs = []\n",
    "        activations = []\n",
    "        for conv in self.convs:\n",
    "            h = torch.relu(conv(emb))  # (B, F, L)\n",
    "            activations.append(h.detach())\n",
    "            pooled = torch.max(h, dim=2)[0]  # (B, F)\n",
    "            conv_outs.append(pooled)\n",
    "        cat = torch.cat(conv_outs, dim=1)\n",
    "        cat = self.dropout(cat)\n",
    "        logits = self.fc(cat)\n",
    "        return logits, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070812b",
   "metadata": {},
   "source": [
    "## 4. Trainings- & Evaluations-Helfer\n",
    "\n",
    "Enthält Epochen-Training (`train_epoch`), Validierung (`eval_model`) sowie Extraktion aktivster n-Grams je Filter zur Interpretierbarkeit (`extract_filter_ngrams`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cae79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits, _ = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_y.extend(yb.cpu().tolist())\n",
    "            all_p.extend(preds.cpu().tolist())\n",
    "    f1 = f1_score(all_y, all_p, average='macro')\n",
    "    acc = accuracy_score(all_y, all_p)\n",
    "    return total_loss / len(loader.dataset), f1, acc, (all_y, all_p)\n",
    "\n",
    "# Interpretability (filter n-grams) based on original logic\n",
    "\n",
    "def extract_filter_ngrams(model, vocab_inv, sample_loader, max_per_filter=2):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in sample_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits, activations = model(xb)\n",
    "            for layer_idx, acts in enumerate(activations):\n",
    "                B,F,L = acts.shape\n",
    "                for f in range(F):\n",
    "                    max_val = -1e9\n",
    "                    best = None\n",
    "                    for b in range(B):\n",
    "                        vals = acts[b,f,:]\n",
    "                        local_max, pos = torch.max(vals, dim=0)\n",
    "                        if local_max.item() > max_val:\n",
    "                            max_val = local_max.item()\n",
    "                            best = (b, pos.item())\n",
    "                    if best is None:\n",
    "                        continue\n",
    "                    b, pos = best\n",
    "                    ksize = model.convs[layer_idx].kernel_size[0]\n",
    "                    token_ids = xb[b, pos:pos+ksize].cpu().tolist()\n",
    "                    tokens = [vocab_inv.get(tid, '<unk>') for tid in token_ids]\n",
    "                    results.append({\n",
    "                        'layer': layer_idx,\n",
    "                        'kernel_size': ksize,\n",
    "                        'filter_index': f,\n",
    "                        'activation': max_val,\n",
    "                        'ngram': tokens\n",
    "                    })\n",
    "            break\n",
    "    grouped = {}\n",
    "    for r in results:\n",
    "        key = (r['layer'], r['filter_index'])\n",
    "        grouped.setdefault(key, []).append(r)\n",
    "    pruned = []\n",
    "    for k,v in grouped.items():\n",
    "        v.sort(key=lambda x: -x['activation'])\n",
    "        pruned.extend(v[:max_per_filter])\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86afa4",
   "metadata": {},
   "source": [
    "## 5. Baseline Lauf\n",
    "\n",
    "Erstellt einen Train/Val/Test Split (70/15/15), baut ein Vokabular mit Standardparametern und trainiert ein einzelnes CNN als Ausgangspunkt. Persistiert Metriken & erste Filter-Beispiele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccab42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_variants = [v for v,f in VARIANT_FILES.items() if (DATA_PROCESSED / f).exists()]\n",
    "assert available_variants, 'No variant files available.'\n",
    "BASE_VARIANT = available_variants[0]\n",
    "print('Baseline variant:', BASE_VARIANT)\n",
    "\n",
    "df_base = load_variant(BASE_VARIANT)\n",
    "text_col, label_col = detect_columns(df_base)\n",
    "# Filter to known classes\n",
    "if label_col == 'severity':\n",
    "    df_base = df_base[df_base[label_col].str.lower().isin(CLASS_NAMES)].copy()\n",
    "labels_raw = df_base[label_col].astype(str).str.lower()\n",
    "label_to_id = {l:i for i,l in enumerate(sorted(labels_raw.unique()))}\n",
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "labels_int = labels_raw.map(label_to_id).values\n",
    "texts = df_base[text_col].fillna('').astype(str).values\n",
    "\n",
    "X_train_txt, X_temp_txt, y_train, y_temp = train_test_split(texts, labels_int, test_size=0.3, stratify=labels_int, random_state=SEED)\n",
    "X_val_txt, X_test_txt, y_val, y_test = train_test_split(X_temp_txt, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
    "print({'train': len(X_train_txt), 'val': len(X_val_txt), 'test': len(X_test_txt)})\n",
    "\n",
    "vocab = build_vocab(X_train_txt, vocab_size=VOCAB_SIZES[0], min_freq=MIN_FREQS[0])\n",
    "vocab_inv = {v:k for k,v in vocab.items()}\n",
    "\n",
    "train_ds = TextDataset(X_train_txt, y_train, vocab, MAX_LEN)\n",
    "val_ds = TextDataset(X_val_txt, y_val, vocab, MAX_LEN)\n",
    "test_ds = TextDataset(X_test_txt, y_test, vocab, MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = CNNTextClassifier(vocab_size=len(vocab), embed_dim=EMBED_DIMS[0], num_classes=len(label_to_id), filter_sizes=FILTER_SIZES, filters_per_size=FILTERS_PER_SIZE, dropout=DROPOUT).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_f1 = -1\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "history = {'train_loss':[], 'val_loss':[], 'train_f1':[], 'val_f1':[], 'train_acc':[], 'val_acc':[]}\n",
    "for ep in range(EPOCHS):\n",
    "    tr_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_f1, val_acc, _ = eval_model(model, val_loader, criterion)\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    # quick train stats (optional compute) - skip extra pass for simplicity\n",
    "    print(f\"Epoch {ep+1}/{EPOCHS} tr_loss={tr_loss:.4f} val_loss={val_loss:.4f} val_f1={val_f1:.4f} val_acc={val_acc:.4f}\")\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# Test evaluation\n",
    "_, test_f1, test_acc, (y_true_test, y_pred_test) = eval_model(model, test_loader, criterion)\n",
    "print('Test macro F1:', test_f1, 'Test Acc:', test_acc)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# Interpretability sample\n",
    "sample_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "filter_examples = extract_filter_ngrams(model, vocab_inv, sample_loader, max_per_filter=2)\n",
    "\n",
    "# Persist single baseline metrics (append or create)\n",
    "metrics_path = RESULTS_DIR / 'metrics_cnn.csv'\n",
    "rec = {\n",
    "    'phase': 'baseline',\n",
    "    'variant': BASE_VARIANT,\n",
    "    'embed_dim': EMBED_DIMS[0],\n",
    "    'vocab_size': VOCAB_SIZES[0],\n",
    "    'min_freq': MIN_FREQS[0],\n",
    "    'macro_f1': best_f1,\n",
    "    'test_macro_f1': test_f1,\n",
    "    'test_accuracy': test_acc,\n",
    "    'n_params': sum(p.numel() for p in model.parameters())\n",
    "}\n",
    "if metrics_path.exists():\n",
    "    dfm = pd.read_csv(metrics_path)\n",
    "    dfm = pd.concat([dfm, pd.DataFrame([rec])], ignore_index=True)\n",
    "else:\n",
    "    dfm = pd.DataFrame([rec])\n",
    "dfm.to_csv(metrics_path, index=False)\n",
    "\n",
    "interpret_path = RESULTS_DIR / 'cnn_filter_activation_examples.json'\n",
    "interpret_data = []\n",
    "if interpret_path.exists():\n",
    "    try:\n",
    "        interpret_data = json.loads(interpret_path.read_text())\n",
    "    except Exception:\n",
    "        interpret_data = []\n",
    "interpret_data.append({'config': rec, 'filter_examples': filter_examples})\n",
    "interpret_path.write_text(json.dumps(interpret_data, indent=2))\n",
    "print('Baseline recorded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb17831",
   "metadata": {},
   "source": [
    "## 6. Parameter-Sweep\n",
    "\n",
    "Iteriert über Varianten und Hyperparameter-Kombinationen (`embed_dim`, `vocab_size`, `min_freq`). Speichert jede Konfiguration inkrementell in `metrics_cnn.csv`. Für die größte Embedding-Dimension werden zusätzlich Filter-n-Gram Beispiele gespeichert. Frühes Stoppen basierend auf Macro-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_records = []\n",
    "metrics_path = RESULTS_DIR / 'metrics_cnn.csv'\n",
    "interpret_path = RESULTS_DIR / 'cnn_filter_activation_examples.json'\n",
    "\n",
    "variants_to_run = [v for v in VARIANT_FILES if (DATA_PROCESSED / VARIANT_FILES[v]).exists()]\n",
    "print('Variants available for sweep:', variants_to_run)\n",
    "\n",
    "for variant in variants_to_run:\n",
    "    dfv = load_variant(variant)\n",
    "    tcol, lcol = detect_columns(dfv)\n",
    "    if lcol == 'severity':\n",
    "        dfv = dfv[dfv[lcol].str.lower().isin(CLASS_NAMES)].copy()\n",
    "    labels_raw = dfv[lcol].astype(str).str.lower()\n",
    "    label_map = {l:i for i,l in enumerate(sorted(labels_raw.unique()))}\n",
    "    inv_label_map = {v:k for k,v in label_map.items()}\n",
    "    y_all = labels_raw.map(label_map).values\n",
    "    texts_all = dfv[tcol].fillna('').astype(str).values\n",
    "    if len(texts_all) < 100:\n",
    "        print('Skip small variant', variant)\n",
    "        continue\n",
    "    X_train_txt, X_val_txt, y_train, y_val = train_test_split(texts_all, y_all, test_size=0.2, stratify=y_all, random_state=SEED)\n",
    "\n",
    "    for embed_dim in EMBED_DIMS:\n",
    "        for vocab_size in VOCAB_SIZES:\n",
    "            for min_freq in MIN_FREQS:\n",
    "                print(f\"Run variant={variant} emb={embed_dim} vocab={vocab_size} min_freq={min_freq}\")\n",
    "                vocab = build_vocab(X_train_txt, vocab_size=vocab_size, min_freq=min_freq)\n",
    "                vocab_inv = {v:k for k,v in vocab.items()}\n",
    "                train_ds = TextDataset(X_train_txt, y_train, vocab, MAX_LEN)\n",
    "                val_ds = TextDataset(X_val_txt, y_val, vocab, MAX_LEN)\n",
    "                train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "                model = CNNTextClassifier(vocab_size=len(vocab), embed_dim=embed_dim, num_classes=len(label_map), filter_sizes=FILTER_SIZES, filters_per_size=FILTERS_PER_SIZE, dropout=DROPOUT).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                best_f1 = -1\n",
    "                best_state = None\n",
    "                epochs_no_improve = 0\n",
    "                start = time.time()\n",
    "                for ep in range(EPOCHS):\n",
    "                    tr_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "                    val_loss, val_f1, val_acc, _ = eval_model(model, val_loader, criterion)\n",
    "                    print(f\"  Ep {ep+1} tr_loss={tr_loss:.3f} val_loss={val_loss:.3f} f1={val_f1:.3f} acc={val_acc:.3f}\")\n",
    "                    if val_f1 > best_f1:\n",
    "                        best_f1 = val_f1\n",
    "                        best_state = model.state_dict()\n",
    "                        epochs_no_improve = 0\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        if epochs_no_improve >= PATIENCE:\n",
    "                            print('  Early stop')\n",
    "                            break\n",
    "                duration = round(time.time()-start,2)\n",
    "                if best_state:\n",
    "                    model.load_state_dict(best_state)\n",
    "                    val_loss, val_f1, val_acc, _ = eval_model(model, val_loader, criterion)\n",
    "                rec = {\n",
    "                    'phase': 'sweep',\n",
    "                    'variant': variant,\n",
    "                    'embed_dim': embed_dim,\n",
    "                    'vocab_size': vocab_size,\n",
    "                    'min_freq': min_freq,\n",
    "                    'macro_f1': val_f1,\n",
    "                    'accuracy': val_acc,\n",
    "                    'train_time_s': duration,\n",
    "                    'n_params': sum(p.numel() for p in model.parameters())\n",
    "                }\n",
    "                sweep_records.append(rec)\n",
    "                # Incremental persistence\n",
    "                if metrics_path.exists():\n",
    "                    dfm = pd.read_csv(metrics_path)\n",
    "                    dfm = pd.concat([dfm, pd.DataFrame([rec])], ignore_index=True)\n",
    "                else:\n",
    "                    dfm = pd.DataFrame([rec])\n",
    "                dfm.to_csv(metrics_path, index=False)\n",
    "                # Interpretability only for largest embedding dim\n",
    "                if embed_dim == max(EMBED_DIMS):\n",
    "                    sample_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "                    filt_examples = extract_filter_ngrams(model, vocab_inv, sample_loader, max_per_filter=2)\n",
    "                    interpret_data = []\n",
    "                    if interpret_path.exists():\n",
    "                        try:\n",
    "                            interpret_data = json.loads(interpret_path.read_text())\n",
    "                        except Exception:\n",
    "                            interpret_data = []\n",
    "                    interpret_data.append({'config': rec, 'filter_examples': filt_examples})\n",
    "                    interpret_path.write_text(json.dumps(interpret_data, indent=2))\n",
    "print('Sweep done; metrics at', metrics_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f4a7f",
   "metadata": {},
   "source": [
    "## 7. Verlauf & Trainingsmetriken (Baseline)\n",
    "\n",
    "Visualisiert Verlaufsdaten (Loss, Accuracy, Macro-F1) des Baseline-Laufs sofern History vorhanden. Sweep-Metriken werden ausschließlich über die aggregierte CSV ausgewertet (separat in Evaluation Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if 'history' in globals() and history['train_loss']:\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(history['train_loss'], label='train_loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.legend(); plt.title('Loss')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(history['val_acc'], label='val_acc')\n",
    "    plt.legend(); plt.title('Val Acc')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(history['val_f1'], label='val_f1')\n",
    "    plt.legend(); plt.title('Val F1')\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('No baseline history to plot.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
